{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install cdsapi  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, cdsapi, typing, zipfile, calendar, multiprocessing\n",
    "from QueryHandler import QueryHandler\n",
    "from shapely.geometry import Point\n",
    "from dotenv import load_dotenv\n",
    "import sqlalchemy as sq \n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../')\n",
    "from DataService import DataService\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "PG_DB = os.getenv('POSTGRES_DB')\n",
    "PG_ADDR = os.getenv('POSTGRES_ADDR')\n",
    "PG_PORT = os.getenv('POSTGRES_PORT')\n",
    "PG_USER = os.getenv('POSTGRES_USER')\n",
    "PG_PW = os.getenv('POSTGRES_PW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 12\n",
    "TABLE = 'copernicus_satelite_data'\n",
    "\n",
    "MIN_MONTH = 3\n",
    "MAX_MONTH = 12\n",
    "\n",
    "MIN_YEAR = 1995\n",
    "MAX_YEAR = 2023\n",
    "\n",
    "years = [str(year) for year in range(MIN_YEAR, MAX_YEAR + 1)]       # the year range we want to pull data from\n",
    "months = [str(month) for month in range(MIN_MONTH, MAX_MONTH + 1)]  # the month range we want to pull data from\n",
    "\n",
    "ATTRS = [                                                           # the attributes we want to pull data for\n",
    "    '2m_dewpoint_temperature', '2m_temperature', 'evaporation_from_bare_soil', 'skin_reservoir_content', 'skin_temperature',\n",
    "    'snowmelt', 'soil_temperature_level_1', 'soil_temperature_level_2', 'soil_temperature_level_3', 'soil_temperature_level_4',\n",
    "    'surface_net_solar_radiation', 'surface_pressure', 'volumetric_soil_water_layer_1', 'volumetric_soil_water_layer_2', \n",
    "    'volumetric_soil_water_layer_3', 'volumetric_soil_water_layer_4'\n",
    "]\n",
    "\n",
    "HOURS = [                                                           # the hours we want to pull data from\n",
    "    '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00', '08:00', '09:00', '10:00', '11:00','12:00', '13:00', \n",
    "    '14:00', '15:00', '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00'\n",
    "]\n",
    "\n",
    "AREA = [61, -125, 48, -88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    db = DataService(PG_DB, PG_ADDR, PG_PORT, PG_USER, PG_PW)       # Handles connections to the database\n",
    "    jobArgs = []                                                    # Holds tuples of arguments for pooled workers\n",
    "    count = 1                                                       # An incrementer used to create unique file names\n",
    "\n",
    "    conn = db.connect()             # Connect to the database\n",
    "    createTable(db)                 # Check the tables, if necessary make a new table for the data\n",
    "    agRegions = loadGeometry(conn)  # Load the agriculture region geometries from the database\n",
    "    db.cleanup()                    # Disconnect from the database (workers maintain their own connections)\n",
    "\n",
    "    # Creates the list of arguments (stored as tuples) used in the multiple processes for pullSateliteData(agRegions, year, month, days, outputFile)\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            numDays = calendar.monthrange(int(year), int(month))[1] # Calculates the number of days - stored in index 1 of a tuple\n",
    "            \n",
    "            days = [str(day) for day in range(1, numDays + 1)]\n",
    "            outputFile = f'copernicus{count}'\n",
    "            count += 1\n",
    "\n",
    "            jobArgs.append(tuple((agRegions, year, month, days, outputFile)))\n",
    "\n",
    "    # Handles the multiple processes\n",
    "    pool = multiprocessing.Pool(NUM_WORKERS)    # Defines the number of workers\n",
    "    pool.starmap(pullSateliteData, jobArgs)     # Creates the queue of jobs - pullSateliteData is the function and jobArgs holds the arguments\n",
    "    pool.close()                                # Once these jobs are finished close the multiple processes pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTable(db: DataService):\n",
    "    queryHandler = QueryHandler()\n",
    "\n",
    "    # check if the copernicus table exists, if it doesnt create it\n",
    "    query = sq.text(queryHandler.tableExistsReq('copernicus_satelite_data'))\n",
    "    tableExists = queryHandler.readTableExists(db.execute(query))\n",
    "    \n",
    "    if not tableExists:\n",
    "        query = sq.text(queryHandler.createCopernicusTableReq())\n",
    "        db.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the agriculture regions from the datbase (projection is EPSG:3347)\n",
    "def loadGeometry(conn: sq.engine.Connection) -> gpd.GeoDataFrame:\n",
    "    query = sq.text('select cr_num, geometry FROM public.census_ag_regions')\n",
    "    agRegions = gpd.GeoDataFrame.from_postgis(query, conn, crs='EPSG:3347', geom_col='geometry')\n",
    "\n",
    "    return agRegions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDateAttrs(df: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    for index, data in df.iterrows():\n",
    "        date = pd.Timestamp(np.datetime64(data['time']))\n",
    "        data['year'] = date.year\n",
    "        data['month'] = date.month\n",
    "        data['day'] = date.day\n",
    "        data['hour'] = date.hour\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addRegions(df: pd.DataFrame, agRegions: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    df = gpd.GeoDataFrame(df, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(df.lon, df.lat)) # Creates geometry from df using lon and lat as cords to create points (points being geometry)\n",
    "    df = df.to_crs(crs='EPSG:3347')                                                         # Changes the points projection to match the agriculture regions of EPSG:3347\n",
    "    df = gpd.sjoin(df, agRegions, how='left', predicate='within')                           # Join the two dataframes based on which points fit within what agriculture regions\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipFile(file: str):\n",
    "    with zipfile.ZipFile(f'{file}.netcdf.zip', 'r') as zip_ref:     # Opens the zip file\n",
    "        zipinfos = zip_ref.infolist()                               # Collects the information of each file contained within\n",
    "\n",
    "        for zipinfo in zipinfos:            # For each file in the zip file (we only expect one)\n",
    "            zipinfo.filename = file         # Changes the unzipped files name (once its unzipped of course)\n",
    "            zip_ref.extract(zipinfo)        # Unzips the file\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNetCDF(file: str) -> pd.DataFrame:\n",
    "    dataset = xr.open_dataset(file)             # Loads the dataset from the netcdf file\n",
    "    df = dataset.to_dataframe().reset_index()   # Converts the contents into a dataframe and corrects indexes \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatDF(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Adds the remaining attributes we want to store which will be gathered during data preprocessing\n",
    "    df['cr_num'] = None\n",
    "    df['year'] = None\n",
    "    df['month'] = None\n",
    "    df['day'] = None\n",
    "    df['hour'] = None\n",
    "\n",
    "    # Renames the dataframes columns so it can be matched when its posted to the database\n",
    "    df.rename(columns={df.columns[0]: 'lon'}, inplace=True)\n",
    "    df.rename(columns={df.columns[1]: 'lat'}, inplace=True)\n",
    "    df.rename(columns={df.columns[2]: 'datetime'}, inplace=True)\n",
    "    df.rename(columns={df.columns[3]: 'dewpoint_temperature'}, inplace=True)\n",
    "    df.rename(columns={df.columns[4]: 'temperature'}, inplace=True)\n",
    "    df.rename(columns={df.columns[5]: 'evaporation_from_bare_soil'}, inplace=True)\n",
    "    df.rename(columns={df.columns[6]: 'skin_reservoir_content'}, inplace=True)\n",
    "    df.rename(columns={df.columns[7]: 'skin_temperature'}, inplace=True)\n",
    "    df.rename(columns={df.columns[8]: 'snowmelt'}, inplace=True)\n",
    "    df.rename(columns={df.columns[9]: 'soil_temperature_level_1'}, inplace=True)\n",
    "    df.rename(columns={df.columns[10]: 'soil_temperature_level_2'}, inplace=True)\n",
    "    df.rename(columns={df.columns[11]: 'soil_temperature_level_3'}, inplace=True)\n",
    "    df.rename(columns={df.columns[12]: 'soil_temperature_level_4'}, inplace=True)\n",
    "    df.rename(columns={df.columns[13]: 'surface_net_solar_radiation'}, inplace=True)\n",
    "    df.rename(columns={df.columns[14]: 'surface_pressure'}, inplace=True)\n",
    "    df.rename(columns={df.columns[15]: 'volumetric_soil_water_layer_1'}, inplace=True)\n",
    "    df.rename(columns={df.columns[16]: 'volumetric_soil_water_layer_2'}, inplace=True)\n",
    "    df.rename(columns={df.columns[17]: 'volumetric_soil_water_layer_3'}, inplace=True)\n",
    "    df.rename(columns={df.columns[18]: 'volumetric_soil_water_layer_4'}, inplace=True)\n",
    "\n",
    "    # Used to detect null values - na.mask, null etc... will be replaced with nan which get removed immediately after\n",
    "    df[['lon', 'lat', 'dewpoint_temperature', 'temperature', 'evaporation_from_bare_soil', 'skin_reservoir_content', 'skin_temperature', 'snowmelt', 'soil_temperature_level_1',\n",
    "        'soil_temperature_level_2', 'soil_temperature_level_3', 'soil_temperature_level_4', 'surface_net_solar_radiation', 'surface_pressure', 'volumetric_soil_water_layer_1',\n",
    "        'volumetric_soil_water_layer_2', 'volumetric_soil_water_layer_3', 'volumetric_soil_water_layer_4']] = df[['lon', 'lat', 'dewpoint_temperature', 'temperature', 'evaporation_from_bare_soil', 'skin_reservoir_content', 'skin_temperature', 'snowmelt', 'soil_temperature_level_1',\n",
    "        'soil_temperature_level_2', 'soil_temperature_level_3', 'soil_temperature_level_4', 'surface_net_solar_radiation', 'surface_pressure', 'volumetric_soil_water_layer_1',\n",
    "        'volumetric_soil_water_layer_2', 'volumetric_soil_water_layer_3', 'volumetric_soil_water_layer_4']].astype(float)\n",
    "\n",
    "    df = df.replace(np.nan, None)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullSateliteData(agRegions: gpd.GeoDataFrame, year: str, month : str, days: list, outputFile: str):\n",
    "    db = DataService(PG_DB, PG_ADDR, PG_PORT, PG_USER, PG_PW)\n",
    "    conn = db.connect()\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    print(f'Starting to pull data for {year}/{month}')\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-land',\n",
    "        {\n",
    "            'format': 'netcdf.zip',\n",
    "            'variable': ATTRS,\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': days,\n",
    "            'time': HOURS,\n",
    "            'area': AREA,\n",
    "        },\n",
    "        f'{outputFile}.netcdf.zip'\n",
    "    )\n",
    "\n",
    "    unzipFile(outputFile)   # Unzips the file, renames it to outputFile and then deletes the source .zip file\n",
    "    \n",
    "    df = readNetCDF(f'{outputFile}.nc') # Converts the netcdf content into a dataframe\n",
    "    df = formatDF(df)                   # Formats the data frame to process null values, add additional attributes and rename columns \n",
    "    df = addRegions(df, agRegions)      # Links data to their crop district number (stored in cr_num)\n",
    "    df = addDateAttrs(df)               # Breaks down the date attributes into its components and saves them for storage\n",
    "    \n",
    "    df.to_sql(TABLE, conn, schema='public', if_exists='append', index=False)\n",
    "\n",
    "    # Clean up the environment after the transaction\n",
    "    os.remove(f'{outputFile}.netcdf.zip')\n",
    "    os.remove(f'{outputFile}.nc')\n",
    "    db.cleanup()\n",
    "                    \n",
    "    print(f'[SUCCESS] data was pulled for {year}/{month}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
