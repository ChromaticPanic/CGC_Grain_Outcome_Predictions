{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install cdsapi  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, cdsapi, typing, zipfile, calendar, multiprocessing\n",
    "from QueryHandler import QueryHandler\n",
    "from shapely.geometry import Point\n",
    "from dotenv import load_dotenv\n",
    "import sqlalchemy as sq \n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../')\n",
    "from DataService import DataService\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "PG_DB = os.getenv('POSTGRES_DB')\n",
    "PG_ADDR = os.getenv('POSTGRES_ADDR')\n",
    "PG_PORT = os.getenv('POSTGRES_PORT')\n",
    "PG_USER = os.getenv('POSTGRES_USER')\n",
    "PG_PW = os.getenv('POSTGRES_PW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 8\n",
    "\n",
    "MIN_MONTH = 3\n",
    "MAX_MONTH = 12\n",
    "\n",
    "MIN_YEAR = 1995\n",
    "MAX_YEAR = 2023\n",
    "\n",
    "years = [str(year) for year in range(MIN_YEAR, MAX_YEAR + 1)]       # the year range we want to pull data from\n",
    "months = [str(month) for month in range(MIN_MONTH, MAX_MONTH + 1)]  # the month range we want to pull data from\n",
    "\n",
    "ATTRS = [                                                           # the attributes we want to pull data for\n",
    "    '2m_dewpoint_temperature', '2m_temperature', 'evaporation_from_bare_soil', 'skin_reservoir_content', 'skin_temperature',\n",
    "    'snowmelt', 'soil_temperature_level_1', 'soil_temperature_level_2', 'soil_temperature_level_3', 'soil_temperature_level_4',\n",
    "    'surface_net_solar_radiation', 'surface_pressure', 'volumetric_soil_water_layer_1', 'volumetric_soil_water_layer_2', \n",
    "    'volumetric_soil_water_layer_3', 'volumetric_soil_water_layer_4'\n",
    "]\n",
    "\n",
    "HOURS = [                                                           # the hours we want to pull data from\n",
    "    '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00', '08:00', '09:00', '10:00', '11:00','12:00', '13:00', \n",
    "    '14:00', '15:00', '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00'\n",
    "]\n",
    "\n",
    "AREA = [61, -125, 48, -88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    db = DataService(PG_DB, PG_ADDR, PG_PORT, PG_USER, PG_PW)\n",
    "    queryHandler = QueryHandler()\n",
    "    jobArgs = [] # create the list of unique jobs -> tuples inside of array\n",
    "    count = 1\n",
    "\n",
    "    conn = db.connect()\n",
    "    createTable(db, queryHandler)   # check the tables, if necessary make a new table for the data\n",
    "    agRegions = loadGeometry(conn)  # load the geometry from the database\n",
    "    db.cleanup()\n",
    "\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            numDays = calendar.monthrange(int(year), int(month))[1]\n",
    "            \n",
    "            days = [str(day) for day in range(1, numDays + 1)]\n",
    "            outputFile = f'copernicus{count}'\n",
    "            count += 1\n",
    "\n",
    "            jobArgs.append(tuple((agRegions, year, month, days, outputFile)))\n",
    "\n",
    "            pullSateliteData(agRegions, year, month, days, outputFile)\n",
    "            break\n",
    "    pool = multiprocessing.Pool(NUM_WORKERS)\n",
    "    pool.starmap(pullSateliteData, jobArgs)\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the copernicus table exists, if it doesnt create it\n",
    "def createTable(db: DataService, queryHandler: QueryHandler):\n",
    "    query = sq.text(queryHandler.tableExistsReq('copernicus_satelite_data'))\n",
    "    tableExists = queryHandler.readTableExists(db.execute(query))\n",
    "    \n",
    "    if not tableExists:\n",
    "        query = sq.text(queryHandler.createCopernicusTableReq())\n",
    "        db.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the agriculture regions from the datbase (projection is EPSG:3347)\n",
    "def loadGeometry(conn: sq.engine.Connection) -> gpd.GeoDataFrame:\n",
    "    query = sq.text('select car_name, geometry FROM public.census_ag_regions')\n",
    "    agRegions = gpd.GeoDataFrame.from_postgis(query, conn, crs='EPSG:3347', geom_col='geometry')\n",
    "\n",
    "    return agRegions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the current point (lon, lat) is in any of the regions pulled from the database\n",
    "def calcAgRegion(agRegions: gpd.GeoDataFrame, point: Point) -> typing.Tuple[int, None]:\n",
    "    area = None   # by default it is assumed we will not find this point\n",
    "\n",
    "    for index, region in agRegions.iterrows():\n",
    "        if region['geometry'].contains(point)[0]:   # for each region, check if the point is within that areas geometry\n",
    "            area = region['cr_num']                 # found it, update the area with the new name\n",
    "            break\n",
    "    \n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeData(df, db: DataService):\n",
    "    queryHandler = QueryHandler()\n",
    "\n",
    "    for index, data in df.iterrows():\n",
    "        date = pd.Timestamp(np.datetime64(data['time']))\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "        day = date.day\n",
    "        hour = date.hour\n",
    "\n",
    "        for col in df.columns:\n",
    "            if np.ma.is_masked(data[col]):\n",
    "                data[col] = 'NULL'\n",
    "\n",
    "        query = sq.text(queryHandler.createInsertRowReq(data, year, month, day, hour))\n",
    "        db.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addRegions(df: pd.DataFrame, agRegions: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    df['cr_num'] = None\n",
    "    \n",
    "    for index, data in df.iterrows():\n",
    "        point = Point(data['longitude'], data['latitude'])  # creates geometry for the current point                      \n",
    "        point = gpd.GeoSeries(point, crs='EPSG:4326')       # transforms the geometry into a geoseries and sets the projection to common (lon, lat)\n",
    "        point = point.to_crs(crs='EPSG:3347')               # changes the points projection to match the agriculture regions\n",
    "        data['cr_num'] = calcAgRegion(agRegions, point)     # adds coordinate region's name (or None if the region is not of interest)    \n",
    "    \n",
    "    df.dropna(subset=['region'], inplace=True)          # drops all rows where region is None\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipFile(file: str):\n",
    "    with zipfile.ZipFile(f'./{file}', 'r') as zip_ref:\n",
    "        zipinfos = zip_ref.infolist()\n",
    "\n",
    "        for zipinfo in zipinfos:\n",
    "            zipinfo.filename = file\n",
    "            zip_ref.extract(zipinfo)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def readNetCDF(file: str) -> pd.DataFrame:\n",
    "    dataset = xr.open_dataset(file)\n",
    "    df = dataset.to_dataframe().reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameCols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.rename(columns={df.columns[3]: 'dewpoint_temperature'}, inplace=True)\n",
    "    df.rename(columns={df.columns[4]: 'temperature'}, inplace=True)\n",
    "    df.rename(columns={df.columns[5]: 'evaporation_from_bare_soil'}, inplace=True)\n",
    "    df.rename(columns={df.columns[6]: 'skin_reservoir_content'}, inplace=True)\n",
    "    df.rename(columns={df.columns[7]: 'skin_temperature'}, inplace=True)\n",
    "    df.rename(columns={df.columns[8]: 'snowmelt'}, inplace=True)\n",
    "    df.rename(columns={df.columns[9]: 'soil_temperature_level_1'}, inplace=True)\n",
    "    df.rename(columns={df.columns[10]: 'soil_temperature_level_2'}, inplace=True)\n",
    "    df.rename(columns={df.columns[11]: 'soil_temperature_level_3'}, inplace=True)\n",
    "    df.rename(columns={df.columns[12]: 'soil_temperature_level_4'}, inplace=True)\n",
    "    df.rename(columns={df.columns[13]: 'surface_net_solar_radiation'}, inplace=True)\n",
    "    df.rename(columns={df.columns[14]: 'surface_pressure'}, inplace=True)\n",
    "    df.rename(columns={df.columns[15]: 'volumetric_soil_water_layer_1'}, inplace=True)\n",
    "    df.rename(columns={df.columns[16]: 'volumetric_soil_water_layer_2'}, inplace=True)\n",
    "    df.rename(columns={df.columns[17]: 'volumetric_soil_water_layer_3'}, inplace=True)\n",
    "    df.rename(columns={df.columns[18]: 'volumetric_soil_water_layer_4'}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullSateliteData(agRegions: gpd.GeoDataFrame, year: str, month : str, days: list, outputFile: str):\n",
    "    db = DataService(PG_DB, PG_ADDR, PG_PORT, PG_USER, PG_PW)\n",
    "    c = cdsapi.Client()\n",
    "    \n",
    "    print(f'Starting to pull data for {year}/{month}')\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-land',\n",
    "        {\n",
    "            'format': 'netcdf.zip',\n",
    "            'variable': ATTRS,\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': days,\n",
    "            'time': HOURS,\n",
    "            'area': AREA,\n",
    "        },\n",
    "        f'{outputFile}.netcdf.zip'\n",
    "    )\n",
    "\n",
    "    unzipFile(outputFile)   # unzips the file, renames it to outputFile and then deletes the source .zip file\n",
    "    \n",
    "    df = readNetCDF(f'{outputFile}.nc')     # converts the netcdf content into a dataframe\n",
    "    df = renameCols(df)                     # renames the columns according to their attribute\n",
    "    df = addRegions(df, agRegions)          # adds regions to all coordinates (data without an associated region is dropped)\n",
    "    storeData(df, db)\n",
    "\n",
    "    # Clean up the environment after the transaction\n",
    "    os.remove(f'{outputFile}.netcdf.zip')\n",
    "    os.remove(f'{outputFile}.nc')\n",
    "    db.cleanup()\n",
    "                    \n",
    "    print(f'[SUCCESS] data was pulled for {year}/{month}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
