{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loadData.ipynb\n",
    "\n",
    "##### Purpose:\n",
    "Download, loads, aggregates and creates final data steps\n",
    "\n",
    "##### Steps:\n",
    "1. Create data folders to house any data files\n",
    "2. Download the data\n",
    "3. Load the data\n",
    "4. Pull data\n",
    "5. Aggregate data\n",
    "6. Build the final data sets\n",
    "\n",
    "##### Remarks:\n",
    "- **Its important to check the settings of each step before its ran, some steps may duplicate their data if ran multiple times**\n",
    "\n",
    "<br>\n",
    "\n",
    "- Both copernicus data and satellite soil moisture data require permission to access their data\n",
    "- Soil data from 1978 - 2001 as well as the ergot data can be found in a one drive to more easily load the data\n",
    "- [Loading data from other sources can be pretty straightforward](#loading-other-data)\n",
    "    - Note that when attempting to load a datasource to a database in this way, sometimes it will not work which requires the following workarounds:\n",
    "        - commiting the change via sqlalchemy\n",
    "        - manually creating the database then inserting the data\n",
    "- [The same can be said for aggregating other data](#aggregating-other-data)\n",
    "- More information about these tables can be found on the [readme](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions)\n",
    "\n",
    "<br>\n",
    "\n",
    "![data download path](./../.github/img/loadData.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datasets.setCreator import SetCreator  # ignore: type\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Create the data subfolders inside each folder found within the source directory\n",
    "\n",
    "Pseudocode:  \n",
    "- Get the current directory (points to src)\n",
    "- Create the would be paths for each data folder\n",
    "- If the data folder does not exist, creat it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcDir = os.getcwd()\n",
    "\n",
    "# Create the would be paths for each data folder\n",
    "datasetsData = os.path.join(srcDir, \"Datasets/data\")\n",
    "ergotData = os.path.join(srcDir, \"Edgot/data\")\n",
    "modelsData = os.path.join(srcDir, \"Models/data\")\n",
    "copernicusData = os.path.join(srcDir, \"SatelliteCopernicus/data\")\n",
    "moistureData = os.path.join(srcDir, \"SatelliteMoisture/data\")\n",
    "sharedData = os.path.join(srcDir, \"Shared/data\")\n",
    "soilData = os.path.join(srcDir, \"Soil/data\")\n",
    "stationData = os.path.join(srcDir, \"WeatherStation/data\")\n",
    "\n",
    "# If the data folder does not exist, creat it\n",
    "if not os.path.exists(datasetsData):\n",
    "    os.makedirs(datasetsData)\n",
    "if not os.path.exists(ergotData):\n",
    "    os.makedirs(ergotData)\n",
    "if not os.path.exists(modelsData):\n",
    "    os.makedirs(modelsData)\n",
    "if not os.path.exists(copernicusData):\n",
    "    os.makedirs(copernicusData)\n",
    "if not os.path.exists(moistureData):\n",
    "    os.makedirs(moistureData)\n",
    "if not os.path.exists(sharedData):\n",
    "    os.makedirs(sharedData)\n",
    "if not os.path.exists(soilData):\n",
    "    os.makedirs(soilData)\n",
    "if not os.path.exists(stationData):\n",
    "    os.makedirs(stationData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Download the province geometries required to load the boarders and stations\n",
    "\n",
    "Pseudocode:  \n",
    "- Move to the stationData directory\n",
    "- [Download the necessary file](https://www150.statcan.gc.ca/n1/pub/92-174-x/2007000/carboundary/gcar000b07a_e.zip)\n",
    "- Extract its contents (since its a zip file)\n",
    "- Go back to src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(stationData)\n",
    "print(os.getcwd())\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    url=\"https://www150.statcan.gc.ca/n1/pub/92-174-x/2007000/carboundary/gcar000b07a_e.zip\",\n",
    "    filename=\"gcar000b07a_e.zip\",\n",
    ")\n",
    "\n",
    "with zipfile.ZipFile(f\"{os.getcwd()}/gcar000b07a_e.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(os.getcwd())\n",
    "\n",
    "os.chdir(f\"{stationData}/../..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Download the list of weather stations\n",
    "\n",
    "Pseudocode:  \n",
    "- Move to the stationData directory\n",
    "- [Download the necessary file](https://dd.weather.gc.ca/climate/observations/climate_station_list.csv)\n",
    "- Go back to src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(stationData)\n",
    "print(os.getcwd())\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    url=\"https://dd.weather.gc.ca/climate/observations/climate_station_list.csv\",\n",
    "    filename=\"climate_station_list.csv\",\n",
    ")\n",
    "\n",
    "os.chdir(f\"{stationData}/../..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Download the soil data\n",
    "\n",
    "Pseudocode:  \n",
    "- Move to the soilData directory\n",
    "- Download the necessary files:\n",
    "    - [geometries and their data](https://sis.agr.gc.ca/nsdb/ca/cac003/cac003.20110308.v3.2/ca_all_slc_v3r2.zip)\n",
    "        - Extract its contents (since its a zip file)\n",
    "    - [Manitoba soil names](https://sis.agr.gc.ca/soildata/mb/soil_name_mb_v2r20130705.dbf)\n",
    "    - [Manitoba soil layers](https://sis.agr.gc.ca/soildata/mb/soil_layer_mb_v2r20130705.dbf)\n",
    "    - [Alberta soil names](https://sis.agr.gc.ca/soildata/ab/soil_name_ab_v2r20140529.dbf)\n",
    "    - [Alberta soil layers](https://sis.agr.gc.ca/soildata/ab/soil_layer_ab_v2r20140529.dbf)\n",
    "    - [Saskatchewan soil names](https://sis.agr.gc.ca/soildata/sk/soil_name_sk_v2r20130705.dbf)\n",
    "    - [Saskatchewan soil layers](https://sis.agr.gc.ca/soildata/sk/soil_layer_sk_v2r20130705.dbf)\n",
    "- Go back to src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(soilData)\n",
    "print(os.getcwd())\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    url=\"https://sis.agr.gc.ca/nsdb/ca/cac003/cac003.20110308.v3.2/ca_all_slc_v3r2.zip\",\n",
    "    filename=\"ca_all_slc_v3r2.zip\",\n",
    ")\n",
    "\n",
    "with zipfile.ZipFile(f\"{os.getcwd()}/ca_all_slc_v3r2.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(os.getcwd())\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    url=\"https://sis.agr.gc.ca/soildata/mb/soil_name_mb_v2r20130705.dbf\",\n",
    "    filename=\"soil_name_mb_v2r20130705.dbf\",\n",
    ")\n",
    "urllib.request.urlretrieve(\n",
    "    url=\"https://sis.agr.gc.ca/soildata/mb/soil_layer_mb_v2r20130705.dbf\",\n",
    "    filename=\"soil_layer_mb_v2r20130705.dbf\",\n",
    ")\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    url=\"https://sis.agr.gc.ca/soildata/ab/soil_name_ab_v2r20140529.dbf\",\n",
    "    filename=\"soil_name_ab_v2r20140529.dbf\",\n",
    ")\n",
    "urllib.request.urlretrieve(\n",
    "    url=\"https://sis.agr.gc.ca/soildata/ab/soil_layer_ab_v2r20140529.dbf\",\n",
    "    filename=\"soil_layer_ab_v2r20140529.dbf\",\n",
    ")\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    url=\"https://sis.agr.gc.ca/soildata/sk/soil_name_sk_v2r20130705.dbf\",\n",
    "    filename=\"soil_name_sk_v2r20130705.dbf\",\n",
    ")\n",
    "urllib.request.urlretrieve(\n",
    "    url=\"https://sis.agr.gc.ca/soildata/sk/soil_layer_sk_v2r20130705.dbf\",\n",
    "    filename=\"soil_layer_sk_v2r20130705.dbf\",\n",
    ")\n",
    "\n",
    "os.chdir(f\"{soilData}/../..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergot data\n",
    "\n",
    "This file was provided by the Canadian Grains Commission. Loading this data must be done manually:\n",
    "\n",
    "1. Locate the file\n",
    "2. Create a copy or move it into the path that is stored in the variable below (ergotData)\n",
    "3. Rename the file to newErgot.csv or change the FILENAME inside of [importErgot](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions/blob/main/src/Ergot/importErgot.ipynb) so that they match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ergotData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soil Moisture data\n",
    "\n",
    "These files were provided by the ESA through means of SFTP, unfortunately we have lost access to these files and therefore they too must be loaded manually:\n",
    "\n",
    "1. Obtain the file either from the University of Manitoba \n",
    "    - stored on woodswallow-01 /../../../../data/common/Images\n",
    "    - stored on Dane's machine\n",
    "2. Create a copy or move it into the path that is stored in the variable below (moistureData)\n",
    "3. Rename the variable MAIN_FOLDER_PATH inside of [PullMoistureData](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions/blob/main/src/SatelliteSoilMoisture/PullMoistureData.py) so that it matches where the files were saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moistureData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Runs the following file to [setup the geometries and stations](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions/blob/main/src/WeatherStation/importBoundariesAndStations.ipynb)\n",
    "\n",
    "Updating the data:\n",
    "- this only needs to be run once\n",
    "- in the event the areas or weather stations change, the script will override whatever was already stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run WeatherStation/importBoundariesAndStations.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Runs the following file to [setup the ergot data](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions/blob/main/src/Ergot/importErgot.ipynb)\n",
    "\n",
    "Updating the data:\n",
    "- this only needs to be run once\n",
    "- in the event the ergot data change, the script will override whatever was already stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Ergot/importErgot.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Runs the following file to [setup the soil moisture data](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions/blob/main/src/SatelliteSoilMoisture/PullMoistureData.py)\n",
    "\n",
    "Updating the data:\n",
    "- this only needs to be run once (data gets appended to the existing table)\n",
    "- in the event more data would like to be added, those [files must be accessed with permission from the European space agency](https://www.esa.int/Applications/Observing_the_Earth/Space_for_our_climate/Nearly_four_decades_of_soil_moisture_data_now_available)\n",
    "\n",
    "Remarks: this file will duplicate its data if ran multiple times unless the data within the soil moisture data folder is moved afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run SatelliteSoilMoisture/PullMoistureData.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Runs the following file to [setup the soil data](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions/blob/main/src/Soil/importSoil.ipynb)\n",
    "\n",
    "Updating the data:\n",
    "- this only needs to be run once \n",
    "- in the event the soil data change, the script will override whatever was already stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Soil/importSoil.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading other data  \n",
    "\n",
    "This part (enclosed within the comment block, should never change)  \n",
    "1. Load the necessary environment variables and packages\n",
    "\n",
    "<br>\n",
    "\n",
    "This part will change depending on the data you load  \n",
    "\n",
    "2. Locate where the data is relative to this file (store in PATH)\n",
    "3. Give a name to the table that will hold the data you are loading (TABLENAME)\n",
    "4. Load it in using one of the two commands: [read_file](https://geopandas.org/en/stable/docs/reference/api/geopandas.read_file.html) or [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    "5. Store it in the database using one of the two commands: [to_sql](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html) or [to_postgis](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.to_postgis.html)\n",
    "\n",
    "Remarks: note that some data will not persist in a database when following these steps, in those instances the following must be done:\n",
    "- [commiting the change via sqlalchemy](https://docs.sqlalchemy.org/en/20/orm/session_basics.html)\n",
    "- manually creating the database then inserting the data [i.e ...](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions/blob/main/src/SatelliteCopernicus/CopernicusQueryBuilder.py)\n",
    "\n",
    "Note this does not include the labeling process either, that can be added by using the following function once loaded:\n",
    "```\n",
    "def addRegions(df: pd.DataFrame, agRegions: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    # Creates geometry from df using lon and lat as cords to create points (points being geometry)\n",
    "    df = gpd.GeoDataFrame(\n",
    "        df, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(df.lon, df.lat)\n",
    "    )\n",
    "\n",
    "    # Changes the points projection to match the agriculture regions of EPSG:3347\n",
    "    df.to_crs(crs=\"EPSG:3347\", inplace=True)  # type: ignore\n",
    "\n",
    "    # Join the two dataframes based on which points fit within what agriculture regions\n",
    "    df = gpd.sjoin(df, agRegions, how=\"left\", predicate=\"within\")\n",
    "\n",
    "    df = pd.DataFrame(df.drop(columns=[\"index_right\", \"geometry\"]))\n",
    "\n",
    "    df = df[df[\"cr_num\"].notna()]  # Take rows that are valid numbers\n",
    "    df[[\"cr_num\"]] = df[[\"cr_num\"]].astype(int)\n",
    "\n",
    "    return df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "#\n",
    "# Load the database connection environment variables located in the docker folder\n",
    "# load_dotenv(\"docker/.env\")\n",
    "# PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "# PG_PW = os.getenv(\"POSTGRES_PW\")\n",
    "# PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "# PG_ADDR = os.getenv(\"POSTGRES_ADDR\")\n",
    "# PG_PORT = os.getenv(\"POSTGRES_PORT\")\n",
    "#\n",
    "# if (\n",
    "#     PG_DB is None\n",
    "#     or PG_ADDR is None\n",
    "#     or PG_PORT is None\n",
    "#     or PG_USER is None\n",
    "#     or PG_PW is None\n",
    "# ):\n",
    "#     raise ValueError(\"Environment variables not set\")\n",
    "#\n",
    "# db = DataService(PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW)\n",
    "# conn = db.connect()\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# PATH = ...\n",
    "# TABLENAME = ...\n",
    "\n",
    "\n",
    "# Data with geometry\n",
    "# ----------------------\n",
    "# data = gpd.read_file(PATH, encoding=\"utf-8\")\n",
    "# data.to_postgis(TABLENAME, conn, index=False, if_exists=\"replace\")\n",
    "\n",
    "# OR\n",
    "\n",
    "# All other data\n",
    "# ----------------------\n",
    "# data = pd.read_csv(PATH)\n",
    "# data.to_sql(TABLENAME, conn, index=False, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Loads the daily weather station data\n",
    "\n",
    "Updating the data: \n",
    "- This script will always pull data from the last year data was pulled for plus any data after that\n",
    "- Stations can be manually disabled by updated their [is_active column](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions/tree/main#station_data_last_updated)\n",
    "- The dates of their latest data is stored in their [latest_data column](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions/tree/main#station_data_last_updated)\n",
    "\n",
    "Remarks: \n",
    "- Data that was already stored gets deleted (no duplicates are ever stored)\n",
    "- Logs are not set up, however if an error is encountered it will be recorded in the console and that particular station would be skipped over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run WeatherStation/scrapeDaily.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Loads the hourly weather station data\n",
    "\n",
    "Updating the data: \n",
    "- This script will always pull all data within the full range of when a weather station has data\n",
    "\n",
    "Remarks: \n",
    "- **Duplicate data may occur if ran multiple times**\n",
    "- Logs are set up, therefore, if an error is encountered it will be recorded and that particular station would be skipped over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run WeatherStation/scrapeHourlyParallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Loads the Copernicus satellite weather data\n",
    "\n",
    "Updating the data: \n",
    "- This script will only pull data that has not been stored in the database yet\n",
    "- Copernicus needs an API key which if access has been/still is gratned can be setup with the [following steps](https://cds.climate.copernicus.eu/api-how-to)\n",
    "\n",
    "Remarks: \n",
    "- Logs are set up, therefore, if an error is encountered it will be recorded and that entry would be skipped over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run SatelliteCopernicus/pullCopernicusData.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Aggregates the ergot data\n",
    "\n",
    "Updating the data: \n",
    "- Rerunning this script will replace previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Ergot/aggregateErgot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Aggregates the ergot data\n",
    "\n",
    "Updating the data: \n",
    "- Rerunning this script will replace previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Ergot/featureEngErgot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Aggregates the weather station data (daily and hourly)\n",
    "\n",
    "Updating the data: \n",
    "- Rerunning this script will replace previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run WeatherStation/CombineProvinceData.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Aggregates the soil data\n",
    "\n",
    "Updating the data: \n",
    "- Rerunning this script will replace previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Soil/soilAggregation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Aggregates the soil moisture data\n",
    "\n",
    "Updating the data: \n",
    "- Rerunning this script will replace previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run SatelliteSoilMoisture/soilMoistureAggregation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregating other data  \n",
    "\n",
    "This part (enclosed within the comment block, should never change)  \n",
    "1. Load the necessary environment variables and packages\n",
    "\n",
    "<br>\n",
    "\n",
    "This part will change depending on the data you load  \n",
    "\n",
    "2. Create the query to load the data from the database (QUERY)\n",
    "3. Decide if the data should be stored as a CSV file or in the database (this will mostly likely depend on how many columns appear in the aggregation)\n",
    "    - If there are more then 1600 use the CSV and set a location to store the aggregated data (PATH)\n",
    "    - Otherwise create a name for the table (TABLENAME)\n",
    "4. [Read the data you want to aggregate](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html)\n",
    "5. [Group the values](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) by whatever you'd like to aggregate the values by and [choose how they should be aggregated](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html)\n",
    "6. Set the column names\n",
    "7. Preprocess the data with the [aggregatorHelper](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions/blob/main/src/Shared/aggregatorHelper.py) if needed\n",
    "8. Store the data\n",
    "    - [CSV](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)\n",
    "    - [Database](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html)\n",
    "\n",
    "Remarks: note that some data will not persist in a database when following these steps, in those instances the following must be done:\n",
    "- [commiting the change via sqlalchemy](https://docs.sqlalchemy.org/en/20/orm/session_basics.html)\n",
    "- manually creating the database then inserting the data [i.e ...](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions/blob/main/src/SatelliteCopernicus/CopernicusQueryBuilder.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# from Shared.aggregatorHelper import AggregatorHelper  #type: ignore\n",
    "# import pandas as pd\n",
    "#\n",
    "# Load the database connection environment variables located in the docker folder\n",
    "# load_dotenv(\"docker/.env\")\n",
    "# PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "# PG_PW = os.getenv(\"POSTGRES_PW\")\n",
    "# PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "# PG_ADDR = os.getenv(\"POSTGRES_ADDR\")\n",
    "# PG_PORT = os.getenv(\"POSTGRES_PORT\")\n",
    "#\n",
    "# if (\n",
    "#     PG_DB is None\n",
    "#     or PG_ADDR is None\n",
    "#     or PG_PORT is None\n",
    "#     or PG_USER is None\n",
    "#     or PG_PW is None\n",
    "# ):\n",
    "#     raise ValueError(\"Environment variables not set\")\n",
    "#\n",
    "# db = DataService(PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW)\n",
    "# conn = db.connect()\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# QUERY = ...\n",
    "# TABLENAME = ...\n",
    "# PATH = ...\n",
    "\n",
    "# data = pd.read_sql(QUERY, conn)\n",
    "\n",
    "# What you group by as well as what aggregated values will depend on the data\n",
    "# agg_df = (\n",
    "#     data.groupby([\"district\", dates of some sort etc... (i.e \"year\", \"week\", \"day\")])\n",
    "#     .agg({\"attribute\": [\"min\", \"max\", \"mean\"]})\n",
    "#     .reset_index()\n",
    "# )\n",
    "\n",
    "# This will set the column names which is again dependent on the data\n",
    "# agg_df.columns = [  # type: ignore\n",
    "#     \"district\",\n",
    "#     dates of some sort etc...\n",
    "#     attributes...\n",
    "# ]\n",
    "\n",
    "# you can either store the aggregated data in the database (if below 1600 columns)\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# agg_df.to_sql(TABLENAME, con=conn, schema=\"public\", if_exists=\"replace\", index=False)\n",
    "\n",
    "# OR\n",
    "\n",
    "# store the aggregated data in a csv file\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# helper = AggregatorHelper()\n",
    "\n",
    "# this example aggregates by individual dates\n",
    "# dates = helper.getDatesInYr()\n",
    "# agg_df = helper.reshapeDataByDates(dates, agg_df, data, \"dates\")\n",
    "\n",
    "# agg_df.to_csv(path_or_buf=PATH, sep=\",\", columns=agg_df.columns.tolist(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Creates exploratory datasets and loads them into a csv file (too many columns for the database)\n",
    "\n",
    "Remarks: these csv files can be found within the DatasetsData folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SetCreator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Creates the final datasets and loads them into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Datasets/DatasetJS.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
