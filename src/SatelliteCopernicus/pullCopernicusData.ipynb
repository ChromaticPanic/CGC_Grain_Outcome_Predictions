{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import calendar\n",
    "import multiprocessing as mp\n",
    "import cdsapi  # type: ignore\n",
    "from dotenv import load_dotenv\n",
    "import sqlalchemy as sq\n",
    "import geopandas as gpd  # type: ignore\n",
    "import xarray as xr  # type: ignore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import black\n",
    "import jupyter_black as bl  # type: ignore\n",
    "from pandas import Index\n",
    "from datetime import datetime\n",
    "from CopernicusQueryBuilder import CopernicusQueryBuilder\n",
    "\n",
    "try:\n",
    "    os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "except:\n",
    "    pass\n",
    "sys.path.append(\"../\")\n",
    "from Shared.DataService import DataService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FILE = \"data/scrape_copernicus_parallel_20230703.log\"\n",
    "ERROR_FILE = \"data/scrape_copernicus_parallel_20230703.err\"\n",
    "\n",
    "bl.load()\n",
    "load_dotenv(\"../docker/.env\")\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "PG_ADDR = os.getenv(\"POSTGRES_ADDR\")\n",
    "PG_PORT = os.getenv(\"POSTGRES_PORT\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PW = os.getenv(\"POSTGRES_PW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 1  # The number of workers we want to employ (maximum is 16 as per the number of cores)\n",
    "REQ_DELAY = 60  # 1 minute - the base delay required to bypass pulling limits\n",
    "MIN_DELAY = 20  # 1 minute - once added to the required delay, creates a minimum delay of 5 minutes to bypass pulling limits\n",
    "MAX_DELAY = 180  # 3 minutes - once added to the required delay, creates a maximum delay of 5 minutes to bypass pulling limits\n",
    "TABLE = \"agg_day_copernicus_satellite_data\"\n",
    "\n",
    "MIN_MONTH = 1\n",
    "MAX_MONTH = 12\n",
    "\n",
    "MIN_YEAR = 1995\n",
    "MAX_YEAR = 2023\n",
    "\n",
    "# the year range we want to pull data from\n",
    "years = [str(year) for year in range(MIN_YEAR, MAX_YEAR + 1)]\n",
    "\n",
    "# the month range we want to pull data from\n",
    "months = [str(month) for month in range(MIN_MONTH, MAX_MONTH + 1)]\n",
    "\n",
    "ATTRS = [  # the attributes we want to pull data for\n",
    "    \"2m_dewpoint_temperature\",\n",
    "    \"2m_temperature\",\n",
    "    \"evaporation_from_bare_soil\",\n",
    "    \"skin_reservoir_content\",\n",
    "    \"skin_temperature\",\n",
    "    \"snowmelt\",\n",
    "    \"soil_temperature_level_1\",\n",
    "    \"soil_temperature_level_2\",\n",
    "    \"soil_temperature_level_3\",\n",
    "    \"soil_temperature_level_4\",\n",
    "    \"surface_net_solar_radiation\",\n",
    "    \"surface_pressure\",\n",
    "    \"volumetric_soil_water_layer_1\",\n",
    "    \"volumetric_soil_water_layer_2\",\n",
    "    \"volumetric_soil_water_layer_3\",\n",
    "    \"volumetric_soil_water_layer_4\",\n",
    "    \"leaf_area_index_high_vegetation\",\n",
    "    \"leaf_area_index_low_vegetation\",\n",
    "]\n",
    "\n",
    "HOURS = [\n",
    "    \"00:00\",\n",
    "    \"01:00\",\n",
    "    \"02:00\",\n",
    "    \"03:00\",\n",
    "    \"04:00\",\n",
    "    \"05:00\",\n",
    "    \"06:00\",\n",
    "    \"07:00\",\n",
    "    \"08:00\",\n",
    "    \"09:00\",\n",
    "    \"10:00\",\n",
    "    \"11:00\",\n",
    "    \"12:00\",\n",
    "    \"13:00\",\n",
    "    \"14:00\",\n",
    "    \"15:00\",\n",
    "    \"16:00\",\n",
    "    \"17:00\",\n",
    "    \"18:00\",\n",
    "    \"19:00\",\n",
    "    \"20:00\",\n",
    "    \"21:00\",\n",
    "    \"22:00\",\n",
    "    \"23:00\",\n",
    "]\n",
    "\n",
    "AREA = [61, -125, 48, -88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateLog(fileName: str, message: str) -> None:\n",
    "    if fileName is not None:\n",
    "        with open(fileName, \"a\") as log:\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            log.write(f\"{timestamp} {message}\")\n",
    "            log.close()\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the agriculture regions from the datbase (projection is EPSG:3347)\n",
    "def loadGeometry(conn: sq.engine.Connection) -> gpd.GeoDataFrame:\n",
    "    query = sq.text(\"select cr_num, district, geometry FROM public.census_ag_regions\")\n",
    "    agRegions = gpd.GeoDataFrame.from_postgis(\n",
    "        query, conn, crs=\"EPSG:3347\", geom_col=\"geometry\"\n",
    "    )\n",
    "\n",
    "    return agRegions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCompleteDates(conn: sq.engine.Connection) -> pd.DataFrame:\n",
    "    \"\"\"year month day\"\"\"\n",
    "    query = sq.text(\n",
    "        \"select distinct year, month, day from agg_day_copernicus_satellite_data\"\n",
    "    )\n",
    "    dates = pd.read_sql(query, conn)\n",
    "\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDateAttrs(df: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    try:\n",
    "        for index in range(len(df.index)):\n",
    "            date = pd.Timestamp(np.datetime64(df.at[index, \"datetime\"]))\n",
    "            df.at[index, \"year\"] = date.year\n",
    "            df.at[index, \"month\"] = date.month\n",
    "            df.at[index, \"day\"] = date.day\n",
    "\n",
    "        df[[\"year\", \"month\", \"day\"]] = df[[\"year\", \"month\", \"day\"]].astype(int)\n",
    "        df.drop(columns=[\"datetime\"], inplace=True)\n",
    "    except Exception as e:\n",
    "        updateLog(ERROR_FILE, f\"Error adding date attributes {e}\\n\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addRegions(df: pd.DataFrame, agRegions: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    # Creates geometry from df using lon and lat as cords to create points (points being geometry)\n",
    "    df = gpd.GeoDataFrame(\n",
    "        df, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(df.lon, df.lat)\n",
    "    )\n",
    "\n",
    "    # Changes the points projection to match the agriculture regions of EPSG:3347\n",
    "    df.to_crs(crs=\"EPSG:3347\", inplace=True)  # type: ignore\n",
    "\n",
    "    # Join the two dataframes based on which points fit within what agriculture regions\n",
    "    df = gpd.sjoin(df, agRegions, how=\"left\", predicate=\"within\")\n",
    "\n",
    "    df.drop(columns=[\"geometry\", \"index_right\", \"lat\", \"lon\"], inplace=True)\n",
    "    df = df[df[\"cr_num\"].notna()]  # Take rows that are valid numbers\n",
    "    df[[\"cr_num\"]] = df[[\"cr_num\"]].astype(int)\n",
    "    df[[\"district\"]] = df[[\"district\"]].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipFile(file: str):\n",
    "    with zipfile.ZipFile(f\"./{file}.netcdf.zip\", \"r\") as zip_ref:  # Opens the zip file\n",
    "        zipinfos = zip_ref.infolist()\n",
    "\n",
    "        for zipinfo in zipinfos:  # we only expect one\n",
    "            zipinfo.filename = f\"{file}.nc\"  # Changes the unzipped files name\n",
    "            zip_ref.extract(zipinfo)  # Unzips the file\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNetCDF(file: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        dataset = xr.open_dataset(f\"./{file}.nc\")\n",
    "        df = dataset.to_dataframe().reset_index()\n",
    "        dataset.close()\n",
    "    except Exception as e:\n",
    "        updateLog(ERROR_FILE, f\"Error reading netCDF file {e}\\n\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatDF(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        # Adds the remaining attributes we want to store which will be gathered during data preprocessing\n",
    "        df[\"year\"] = None\n",
    "        df[\"month\"] = None\n",
    "        df[\"day\"] = None\n",
    "\n",
    "        # Renames the dataframes columns so it can be matched when its posted to the database\n",
    "        df.rename(columns={\"longitude\": \"lon\"}, inplace=True)\n",
    "        df.rename(columns={\"latitude\": \"lat\"}, inplace=True)\n",
    "        df.rename(columns={\"time\": \"datetime\"}, inplace=True)\n",
    "        df.rename(columns={\"d2m\": \"dewpoint_temperature\"}, inplace=True)\n",
    "        df.rename(columns={\"t2m\": \"temperature\"}, inplace=True)\n",
    "        df.rename(columns={\"evabs\": \"evaporation_from_bare_soil\"}, inplace=True)\n",
    "        df.rename(columns={\"src\": \"skin_reservoir_content\"}, inplace=True)\n",
    "        df.rename(columns={\"skt\": \"skin_temperature\"}, inplace=True)\n",
    "        df.rename(columns={\"smlt\": \"snowmelt\"}, inplace=True)\n",
    "        df.rename(columns={\"stl1\": \"soil_temperature_level_1\"}, inplace=True)\n",
    "        df.rename(columns={\"stl2\": \"soil_temperature_level_2\"}, inplace=True)\n",
    "        df.rename(columns={\"stl3\": \"soil_temperature_level_3\"}, inplace=True)\n",
    "        df.rename(columns={\"stl4\": \"soil_temperature_level_4\"}, inplace=True)\n",
    "        df.rename(columns={\"ssr\": \"surface_net_solar_radiation\"}, inplace=True)\n",
    "        df.rename(columns={\"sp\": \"surface_pressure\"}, inplace=True)\n",
    "        df.rename(columns={\"swvl1\": \"volumetric_soil_water_layer_1\"}, inplace=True)\n",
    "        df.rename(columns={\"swvl2\": \"volumetric_soil_water_layer_2\"}, inplace=True)\n",
    "        df.rename(columns={\"swvl3\": \"volumetric_soil_water_layer_3\"}, inplace=True)\n",
    "        df.rename(columns={\"swvl4\": \"volumetric_soil_water_layer_4\"}, inplace=True)\n",
    "        df.rename(columns={\"lai_hv\": \"leaf_area_index_high_vegetation\"}, inplace=True)\n",
    "        df.rename(columns={\"lai_lv\": \"leaf_area_index_low_vegetation\"}, inplace=True)\n",
    "\n",
    "        # if column is not in the dataframe then add it with a null value\n",
    "        columns = [\n",
    "            \"lon\",\n",
    "            \"lat\",\n",
    "            \"dewpoint_temperature\",\n",
    "            \"temperature\",\n",
    "            \"evaporation_from_bare_soil\",\n",
    "            \"skin_reservoir_content\",\n",
    "            \"skin_temperature\",\n",
    "            \"snowmelt\",\n",
    "            \"soil_temperature_level_1\",\n",
    "            \"soil_temperature_level_2\",\n",
    "            \"soil_temperature_level_3\",\n",
    "            \"soil_temperature_level_4\",\n",
    "            \"surface_net_solar_radiation\",\n",
    "            \"surface_pressure\",\n",
    "            \"volumetric_soil_water_layer_1\",\n",
    "            \"volumetric_soil_water_layer_2\",\n",
    "            \"volumetric_soil_water_layer_3\",\n",
    "            \"volumetric_soil_water_layer_4\",\n",
    "            \"leaf_area_index_high_vegetation\",\n",
    "            \"leaf_area_index_low_vegetation\",\n",
    "        ]\n",
    "        for column in columns:\n",
    "            if column not in df.columns:\n",
    "                df[column] = None\n",
    "\n",
    "        # Used to detect null values - na.mask, null etc... will be replaced with nan which get removed immediately after\n",
    "        df[columns] = df[columns].astype(float)\n",
    "    except Exception as e:\n",
    "        updateLog(ERROR_FILE, f\"Error formatting dataframe {e}\\n\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDailyAggregate(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        aggregate = (\n",
    "            df.groupby([\"year\", \"month\", \"day\", \"cr_num\", \"district\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"dewpoint_temperature\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"temperature\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"evaporation_from_bare_soil\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"skin_reservoir_content\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"skin_temperature\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"snowmelt\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"soil_temperature_level_1\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"soil_temperature_level_2\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"soil_temperature_level_3\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"soil_temperature_level_4\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"surface_net_solar_radiation\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"surface_pressure\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"volumetric_soil_water_layer_1\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"volumetric_soil_water_layer_2\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"volumetric_soil_water_layer_3\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"volumetric_soil_water_layer_4\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"leaf_area_index_high_vegetation\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"leaf_area_index_low_vegetation\": [\"min\", \"max\", \"mean\"],\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        columns = [\n",
    "            \"min_dewpoint_temperature\",\n",
    "            \"max_dewpoint_temperature\",\n",
    "            \"mean_dewpoint_temperature\",\n",
    "            \"min_temperature\",\n",
    "            \"max_temperature\",\n",
    "            \"mean_temperature\",\n",
    "            \"min_evaporation_from_bare_soil\",\n",
    "            \"max_evaporation_from_bare_soil\",\n",
    "            \"mean_evaporation_from_bare_soil\",\n",
    "            \"min_skin_reservoir_content\",\n",
    "            \"max_skin_reservoir_content\",\n",
    "            \"mean_skin_reservoir_content\",\n",
    "            \"min_skin_temperature\",\n",
    "            \"max_skin_temperature\",\n",
    "            \"mean_skin_temperature\",\n",
    "            \"min_snowmelt\",\n",
    "            \"max_snowmelt\",\n",
    "            \"mean_snowmelt\",\n",
    "            \"min_soil_temperature_level_1\",\n",
    "            \"max_soil_temperature_level_1\",\n",
    "            \"mean_soil_temperature_level_1\",\n",
    "            \"min_soil_temperature_level_2\",\n",
    "            \"max_soil_temperature_level_2\",\n",
    "            \"mean_soil_temperature_level_2\",\n",
    "            \"min_soil_temperature_level_3\",\n",
    "            \"max_soil_temperature_level_3\",\n",
    "            \"mean_soil_temperature_level_3\",\n",
    "            \"min_soil_temperature_level_4\",\n",
    "            \"max_soil_temperature_level_4\",\n",
    "            \"mean_soil_temperature_level_4\",\n",
    "            \"min_surface_net_solar_radiation\",\n",
    "            \"max_surface_net_solar_radiation\",\n",
    "            \"mean_surface_net_solar_radiation\",\n",
    "            \"min_surface_pressure\",\n",
    "            \"max_surface_pressure\",\n",
    "            \"mean_surface_pressure\",\n",
    "            \"min_volumetric_soil_water_layer_1\",\n",
    "            \"max_volumetric_soil_water_layer_1\",\n",
    "            \"mean_volumetric_soil_water_layer_1\",\n",
    "            \"min_volumetric_soil_water_layer_2\",\n",
    "            \"max_volumetric_soil_water_layer_2\",\n",
    "            \"mean_volumetric_soil_water_layer_2\",\n",
    "            \"min_volumetric_soil_water_layer_3\",\n",
    "            \"max_volumetric_soil_water_layer_3\",\n",
    "            \"mean_volumetric_soil_water_layer_3\",\n",
    "            \"min_volumetric_soil_water_layer_4\",\n",
    "            \"max_volumetric_soil_water_layer_4\",\n",
    "            \"mean_volumetric_soil_water_layer_4\",\n",
    "            \"min_leaf_area_index_high_vegetation\",\n",
    "            \"max_leaf_area_index_high_vegetation\",\n",
    "            \"mean_leaf_area_index_high_vegetation\",\n",
    "            \"min_leaf_area_index_low_vegetation\",\n",
    "            \"max_leaf_area_index_low_vegetation\",\n",
    "            \"mean_leaf_area_index_low_vegetation\",\n",
    "        ]\n",
    "        all_columns = [\n",
    "            \"year\",\n",
    "            \"month\",\n",
    "            \"day\",\n",
    "            \"cr_num\",\n",
    "            \"district\",\n",
    "        ]\n",
    "        all_columns.extend(columns)\n",
    "        aggregate.columns = Index(all_columns)\n",
    "        aggregate[columns] = aggregate[columns].astype(float)\n",
    "    except Exception as e:\n",
    "        updateLog(ERROR_FILE, f\"Error generating daily aggregate {e}\\n\")\n",
    "\n",
    "    return aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullSatelliteData(\n",
    "    agRegions: gpd.GeoDataFrame,\n",
    "    delay: int,\n",
    "    year: str,\n",
    "    month: str,\n",
    "    days: list,\n",
    "    outputFile: str,\n",
    "):\n",
    "    try:\n",
    "        os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "    except:\n",
    "        pass\n",
    "    if (\n",
    "        PG_DB is None\n",
    "        or PG_ADDR is None\n",
    "        or PG_PORT is None\n",
    "        or PG_USER is None\n",
    "        or PG_PW is None\n",
    "    ):\n",
    "        updateLog(ERROR_FILE, \"Environment variables not set\")\n",
    "        raise ValueError(\"Environment variables not set\")\n",
    "\n",
    "    db = DataService(PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW)\n",
    "    time.sleep(delay)\n",
    "\n",
    "    conn = db.connect()\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    for currDay in days:\n",
    "        currFile = f\"{outputFile}_{currDay}\"\n",
    "        try:\n",
    "            starttime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            updateLog(\n",
    "                LOG_FILE,\n",
    "                f\"{starttime} Starting to pull data for {year}/{month}/{currDay}\\n\",\n",
    "            )\n",
    "            c.retrieve(\n",
    "                \"reanalysis-era5-land\",\n",
    "                {\n",
    "                    \"format\": \"netcdf.zip\",\n",
    "                    \"variable\": ATTRS,\n",
    "                    \"year\": year,\n",
    "                    \"month\": month,\n",
    "                    \"day\": currDay,\n",
    "                    \"time\": HOURS,\n",
    "                    \"area\": AREA,\n",
    "                },\n",
    "                f\"{currFile}.netcdf.zip\",\n",
    "            )\n",
    "\n",
    "            # Unzips the file, renames it to outputFile and then deletes the source .zip file\n",
    "            unzipFile(currFile)\n",
    "\n",
    "            df = readNetCDF(currFile)  # Converts the netcdf content into a dataframe\n",
    "            # Formats the data frame to process null values, add additional attributes and rename columns\n",
    "\n",
    "            df = formatDF(df)\n",
    "\n",
    "            updateLog(\n",
    "                LOG_FILE,\n",
    "                f\"Starting to match regions for data in {year}/{month}/{currDay}\\n\",\n",
    "            )\n",
    "            # Links data to their crop district number (stored in cr_num)\n",
    "            df = addRegions(df, agRegions)\n",
    "            df = df.reset_index()\n",
    "            updateLog(\n",
    "                LOG_FILE,\n",
    "                f\"Finished matching {len(df.index)} regions for data in {year}/{month}/{currDay}\\n\",\n",
    "            )\n",
    "\n",
    "            # Breaks down the date attributes into its components and saves them for storage\n",
    "            df = addDateAttrs(df)\n",
    "            updateLog(LOG_FILE, f\"Added date attributes for {year}/{month}/{currDay}\\n\")\n",
    "\n",
    "            df = generateDailyAggregate(df)\n",
    "\n",
    "            updateLog(\n",
    "                LOG_FILE,\n",
    "                f\"Adding rows {len(df.index)} data from {year}/{month}/{currDay} to the Database\\n\",\n",
    "            )\n",
    "            # df.drop(columns=[\"index\"], inplace=True)\n",
    "            df.to_sql(TABLE, conn, schema=\"public\", if_exists=\"append\", index=False)\n",
    "            updateLog(\n",
    "                LOG_FILE,\n",
    "                f\"{starttime} Finished data pull from {year}/{month}/{currDay}\\n\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            updateLog(\n",
    "                ERROR_FILE, f\"Error pulling data for {year}/{month}/{currDay} : {e}\\n\"\n",
    "            )\n",
    "\n",
    "        # Clean up the environment after the transaction\n",
    "        try:\n",
    "            os.remove(f\"{currFile}.nc\")\n",
    "            os.remove(f\"{currFile}.netcdf.zip\")\n",
    "        except Exception as e:\n",
    "            updateLog(ERROR_FILE, f\"Error cleaning up {year}/{month}/{currDay} : {e}\\n\")\n",
    "\n",
    "    db.cleanup()\n",
    "    updateLog(LOG_FILE, f\"Finished adding {year}/{month} to the Database\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if (\n",
    "        PG_DB is None\n",
    "        or PG_ADDR is None\n",
    "        or PG_PORT is None\n",
    "        or PG_USER is None\n",
    "        or PG_PW is None\n",
    "    ):\n",
    "        raise ValueError(\"Environment variables not set\")\n",
    "\n",
    "    # Handles connections to the database\n",
    "    db = DataService(PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW)\n",
    "    queryHandler = CopernicusQueryBuilder()\n",
    "    jobArgs = []  # Holds tuples of arguments for pooled workers\n",
    "\n",
    "    conn = db.connect()  # Connect to the database\n",
    "    queryHandler.createCopernicusTableReq(db)\n",
    "\n",
    "    # Load the agriculture region geometries from the database\n",
    "    agRegions = loadGeometry(conn)\n",
    "\n",
    "    completedDf = getCompleteDates(conn)\n",
    "    db.cleanup()  # Disconnect from the database (workers maintain their own connections)\n",
    "\n",
    "    # Creates the list of arguments (stored as tuples) used in the multiple processes for pullSateliteData(agRegions, year, month, days, outputFile)\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            # Calculates the number of days - stored in index 1 of a tuple\n",
    "            numDays = calendar.monthrange(int(year), int(month))[1]\n",
    "\n",
    "            # calculates a random delay (asc for groups of 12)\n",
    "            delay = random.randint(MIN_DELAY, MAX_DELAY)\n",
    "\n",
    "            days = [str(day) for day in range(1, numDays + 1)]\n",
    "            outputFile = f\"data/copernicus_{year}_{month}\"\n",
    "\n",
    "            # removes the days that have already been completed\n",
    "            completedDaysDf = completedDf.loc[\n",
    "                (completedDf[\"year\"] == int(year))\n",
    "                & (completedDf[\"month\"] == int(month))\n",
    "            ]\n",
    "            completedDays = completedDaysDf[\"day\"].tolist()\n",
    "            incompleteDays = []\n",
    "            for day in days:\n",
    "                if int(day) not in completedDays:\n",
    "                    incompleteDays.append(day)\n",
    "\n",
    "            if len(incompleteDays) > 0:\n",
    "                jobArgs.append(\n",
    "                    tuple((agRegions, delay, year, month, incompleteDays, outputFile))\n",
    "                )\n",
    "\n",
    "    # Handles the multiple processes\n",
    "    pool = mp.Pool(NUM_WORKERS)  # Defines the number of workers\n",
    "\n",
    "    # Creates the queue of jobs - pullSateliteData is the function and jobArgs holds the arguments\n",
    "    pool.starmap(pullSatelliteData, jobArgs)\n",
    "    pool.close()  # Once these jobs are finished close the multiple processes pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to match regions for data in 1995/1/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
