{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when running this notebook or the associated script it is worthwhile loading the output to a file like so: python pullCopernicusData.py > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, random, zipfile, calendar, multiprocessing\n",
    "import cdsapi  # type: ignore\n",
    "from CopernicusQueryBuilder import CopernicusQueryBuilder\n",
    "from dotenv import load_dotenv\n",
    "import sqlalchemy as sq\n",
    "import geopandas as gpd  # type: ignore\n",
    "import xarray as xr  # type: ignore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from Shared.DataService import DataService # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "PG_ADDR = os.getenv(\"POSTGRES_ADDR\")\n",
    "PG_PORT = os.getenv(\"POSTGRES_PORT\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PW = os.getenv(\"POSTGRES_PW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 12  # The number of workers we want to employ (maximum is 16 as per the number of cores)\n",
    "REQ_DELAY = 60  # 1 minute - the base delay required to bypass pulling limits\n",
    "MIN_DELAY = 60  # 1 minute - once added to the required delay, creates a minimum delay of 5 minutes to bypass pulling limits\n",
    "MAX_DELAY = 180  # 3 minutes - once added to the required delay, creates a maximum delay of 5 minutes to bypass pulling limits\n",
    "TABLE = \"copernicus_satelite_data\"\n",
    "\n",
    "MIN_MONTH = 1\n",
    "MAX_MONTH = 12\n",
    "\n",
    "MIN_YEAR = 1995\n",
    "MAX_YEAR = 2023\n",
    "\n",
    "years = [\n",
    "    str(year) for year in range(MIN_YEAR, MAX_YEAR + 1)\n",
    "]  # the year range we want to pull data from\n",
    "months = [\n",
    "    str(month) for month in range(MIN_MONTH, MAX_MONTH + 1)\n",
    "]  # the month range we want to pull data from\n",
    "\n",
    "ATTRS = [  # the attributes we want to pull data for\n",
    "    \"2m_dewpoint_temperature\",\n",
    "    \"2m_temperature\",\n",
    "    \"evaporation_from_bare_soil\",\n",
    "    \"skin_reservoir_content\",\n",
    "    \"skin_temperature\",\n",
    "    \"snowmelt\",\n",
    "    \"soil_temperature_level_1\",\n",
    "    \"soil_temperature_level_2\",\n",
    "    \"soil_temperature_level_3\",\n",
    "    \"soil_temperature_level_4\",\n",
    "    \"surface_net_solar_radiation\",\n",
    "    \"surface_pressure\",\n",
    "    \"volumetric_soil_water_layer_1\",\n",
    "    \"volumetric_soil_water_layer_2\",\n",
    "    \"volumetric_soil_water_layer_3\",\n",
    "    \"volumetric_soil_water_layer_4\",\n",
    "    \"leaf_area_index_high_vegetation\",\n",
    "    \"leaf_area_index_low_vegetation\",\n",
    "]\n",
    "\n",
    "HOURS = [\n",
    "    \"00:00\",\n",
    "    \"01:00\",\n",
    "    \"02:00\",\n",
    "    \"03:00\",\n",
    "    \"04:00\",\n",
    "    \"05:00\",\n",
    "    \"06:00\",\n",
    "    \"07:00\",\n",
    "    \"08:00\",\n",
    "    \"09:00\",\n",
    "    \"10:00\",\n",
    "    \"11:00\",\n",
    "    \"12:00\",\n",
    "    \"13:00\",\n",
    "    \"14:00\",\n",
    "    \"15:00\",\n",
    "    \"16:00\",\n",
    "    \"17:00\",\n",
    "    \"18:00\",\n",
    "    \"19:00\",\n",
    "    \"20:00\",\n",
    "    \"21:00\",\n",
    "    \"22:00\",\n",
    "    \"23:00\",\n",
    "]\n",
    "\n",
    "AREA = [61, -125, 48, -88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if (\n",
    "        PG_DB is None\n",
    "        or PG_ADDR is None\n",
    "        or PG_PORT is None\n",
    "        or PG_USER is None\n",
    "        or PG_PW is None\n",
    "    ):\n",
    "        raise ValueError(\"Environment variables not set\")\n",
    "\n",
    "    db = DataService(\n",
    "        PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW\n",
    "    )  # Handles connections to the database\n",
    "    queryHandler = CopernicusQueryBuilder()\n",
    "    jobArgs = []  # Holds tuples of arguments for pooled workers\n",
    "    count = 1  # An incrementer used to create unique file names\n",
    "\n",
    "    conn = db.connect()  # Connect to the database\n",
    "    queryHandler.createCopernicusTableReq(db)\n",
    "    agRegions = loadGeometry(\n",
    "        conn\n",
    "    )  # Load the agriculture region geometries from the database\n",
    "    db.cleanup()  # Disconnect from the database (workers maintain their own connections)\n",
    "\n",
    "    # Creates the list of arguments (stored as tuples) used in the multiple processes for pullSateliteData(agRegions, year, month, days, outputFile)\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            numDays = calendar.monthrange(int(year), int(month))[\n",
    "                1\n",
    "            ]  # Calculates the number of days - stored in index 1 of a tuple\n",
    "            delay = (count % NUM_WORKERS != 0) * (\n",
    "                REQ_DELAY * (count % NUM_WORKERS) + random.randint(MIN_DELAY, MAX_DELAY)\n",
    "            )  # calculates a random delay (asc for groups of 12)\n",
    "\n",
    "            days = [str(day) for day in range(1, numDays + 1)]\n",
    "            outputFile = f\"copernicus{count}\"\n",
    "            count += 1\n",
    "\n",
    "            jobArgs.append(tuple((agRegions, delay, year, month, days, outputFile)))\n",
    "\n",
    "    # Handles the multiple processes\n",
    "    pool = multiprocessing.Pool(NUM_WORKERS)  # Defines the number of workers\n",
    "    pool.starmap(\n",
    "        pullSatelliteData, jobArgs\n",
    "    )  # Creates the queue of jobs - pullSateliteData is the function and jobArgs holds the arguments\n",
    "    pool.close()  # Once these jobs are finished close the multiple processes pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the agriculture regions from the datbase (projection is EPSG:3347)\n",
    "def loadGeometry(conn: sq.engine.Connection) -> gpd.GeoDataFrame:\n",
    "    query = sq.text(\"select cr_num, geometry FROM public.census_ag_regions\")\n",
    "    agRegions = gpd.GeoDataFrame.from_postgis(\n",
    "        query, conn, crs=\"EPSG:3347\", geom_col=\"geometry\"\n",
    "    )\n",
    "\n",
    "    return agRegions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDateAttrs(df: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    try:\n",
    "        for index in range(len(df.index)):\n",
    "            date = pd.Timestamp(np.datetime64(df.at[index, \"datetime\"]))\n",
    "            df.at[index, \"year\"] = date.year\n",
    "            df.at[index, \"month\"] = date.month\n",
    "            df.at[index, \"day\"] = date.day\n",
    "            df.at[index, \"hour\"] = date.hour\n",
    "\n",
    "        df[[\"year\", \"month\", \"day\", \"hour\"]] = df[\n",
    "            [\"year\", \"month\", \"day\", \"hour\"]\n",
    "        ].astype(int)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addRegions(df: pd.DataFrame, agRegions: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    df = gpd.GeoDataFrame(\n",
    "        df, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(df.lon, df.lat)\n",
    "    )  # Creates geometry from df using lon and lat as cords to create points (points being geometry)\n",
    "    df.to_crs( # type: ignore\n",
    "        crs=\"EPSG:3347\", inplace=True\n",
    "    )  # Changes the points projection to match the agriculture regions of EPSG:3347\n",
    "    df = gpd.sjoin(\n",
    "        df, agRegions, how=\"left\", predicate=\"within\"\n",
    "    )  # Join the two dataframes based on which points fit within what agriculture regions\n",
    "\n",
    "    df.drop(columns=[\"geometry\", \"index_right\"], inplace=True)\n",
    "    df = df[df[\"cr_num\"].notna()]  # Take rows that are valid numbers\n",
    "    df[[\"cr_num\"]] = df[[\"cr_num\"]].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipFile(file: str):\n",
    "    with zipfile.ZipFile(f\"./{file}.netcdf.zip\", \"r\") as zip_ref:  # Opens the zip file\n",
    "        zipinfos = (\n",
    "            zip_ref.infolist()\n",
    "        )  # Collects the information of each file contained within\n",
    "\n",
    "        for zipinfo in zipinfos:  # For each file in the zip file (we only expect one)\n",
    "            zipinfo.filename = f\"{file}.nc\"  # Changes the unzipped files name (once its unzipped of course)\n",
    "            zip_ref.extract(zipinfo)  # Unzips the file\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNetCDF(file: str) -> pd.DataFrame:\n",
    "    dataset = xr.open_dataset(f\"./{file}.nc\")  # Loads the dataset from the netcdf file\n",
    "    df = (\n",
    "        dataset.to_dataframe().reset_index()\n",
    "    )  # Converts the contents into a dataframe and corrects indexes\n",
    "\n",
    "    dataset.close()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatDF(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Adds the remaining attributes we want to store which will be gathered during data preprocessing\n",
    "    df[\"year\"] = None\n",
    "    df[\"month\"] = None\n",
    "    df[\"day\"] = None\n",
    "    df[\"hour\"] = None\n",
    "\n",
    "    # Renames the dataframes columns so it can be matched when its posted to the database\n",
    "    df.rename(columns={df.columns[0]: \"lon\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[1]: \"lat\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[2]: \"datetime\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[3]: \"dewpoint_temperature\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[4]: \"temperature\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[5]: \"evaporation_from_bare_soil\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[6]: \"skin_reservoir_content\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[7]: \"skin_temperature\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[8]: \"snowmelt\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[9]: \"soil_temperature_level_1\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[10]: \"soil_temperature_level_2\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[11]: \"soil_temperature_level_3\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[12]: \"soil_temperature_level_4\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[13]: \"surface_net_solar_radiation\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[14]: \"surface_pressure\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[15]: \"volumetric_soil_water_layer_1\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[16]: \"volumetric_soil_water_layer_2\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[17]: \"volumetric_soil_water_layer_3\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[18]: \"volumetric_soil_water_layer_4\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[19]: \"leaf_area_index_high_vegetation\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[20]: \"leaf_area_index_low_vegetation\"}, inplace=True)\n",
    "\n",
    "    # Used to detect null values - na.mask, null etc... will be replaced with nan which get removed immediately after\n",
    "    df[\n",
    "        [\n",
    "            \"lon\",\n",
    "            \"lat\",\n",
    "            \"dewpoint_temperature\",\n",
    "            \"temperature\",\n",
    "            \"evaporation_from_bare_soil\",\n",
    "            \"skin_reservoir_content\",\n",
    "            \"skin_temperature\",\n",
    "            \"snowmelt\",\n",
    "            \"soil_temperature_level_1\",\n",
    "            \"soil_temperature_level_2\",\n",
    "            \"soil_temperature_level_3\",\n",
    "            \"soil_temperature_level_4\",\n",
    "            \"surface_net_solar_radiation\",\n",
    "            \"surface_pressure\",\n",
    "            \"volumetric_soil_water_layer_1\",\n",
    "            \"volumetric_soil_water_layer_2\",\n",
    "            \"volumetric_soil_water_layer_3\",\n",
    "            \"volumetric_soil_water_layer_4\",\n",
    "            \"leaf_area_index_high_vegetation\",\n",
    "            \"leaf_area_index_low_vegetation\",\n",
    "        ]\n",
    "    ] = df[\n",
    "        [\n",
    "            \"lon\",\n",
    "            \"lat\",\n",
    "            \"dewpoint_temperature\",\n",
    "            \"temperature\",\n",
    "            \"evaporation_from_bare_soil\",\n",
    "            \"skin_reservoir_content\",\n",
    "            \"skin_temperature\",\n",
    "            \"snowmelt\",\n",
    "            \"soil_temperature_level_1\",\n",
    "            \"soil_temperature_level_2\",\n",
    "            \"soil_temperature_level_3\",\n",
    "            \"soil_temperature_level_4\",\n",
    "            \"surface_net_solar_radiation\",\n",
    "            \"surface_pressure\",\n",
    "            \"volumetric_soil_water_layer_1\",\n",
    "            \"volumetric_soil_water_layer_2\",\n",
    "            \"volumetric_soil_water_layer_3\",\n",
    "            \"volumetric_soil_water_layer_4\",\n",
    "            \"leaf_area_index_high_vegetation\",\n",
    "            \"leaf_area_index_low_vegetation\",\n",
    "        ]\n",
    "    ].astype(\n",
    "        float\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullSatelliteData(\n",
    "    agRegions: gpd.GeoDataFrame,\n",
    "    delay: int,\n",
    "    year: str,\n",
    "    month: str,\n",
    "    days: list,\n",
    "    outputFile: str,\n",
    "):\n",
    "    if (\n",
    "        PG_DB is None\n",
    "        or PG_ADDR is None\n",
    "        or PG_PORT is None\n",
    "        or PG_USER is None\n",
    "        or PG_PW is None\n",
    "    ):\n",
    "        raise ValueError(\"Environment variables not set\")\n",
    "\n",
    "    db = DataService(PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW)\n",
    "    time.sleep(delay)\n",
    "\n",
    "    print(f\"Starting to pull data for {year}/{month}\")\n",
    "    conn = db.connect()\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    try:\n",
    "        c.retrieve(\n",
    "            \"reanalysis-era5-land\",\n",
    "            {\n",
    "                \"format\": \"netcdf.zip\",\n",
    "                \"variable\": ATTRS,\n",
    "                \"year\": year,\n",
    "                \"month\": month,\n",
    "                \"day\": days,\n",
    "                \"time\": HOURS,\n",
    "                \"area\": AREA,\n",
    "            },\n",
    "            f\"{outputFile}.netcdf.zip\",\n",
    "        )\n",
    "\n",
    "        unzipFile(\n",
    "            outputFile\n",
    "        )  # Unzips the file, renames it to outputFile and then deletes the source .zip file\n",
    "\n",
    "        df = readNetCDF(outputFile)  # Converts the netcdf content into a dataframe\n",
    "        df = formatDF(\n",
    "            df\n",
    "        )  # Formats the data frame to process null values, add additional attributes and rename columns\n",
    "\n",
    "        print(f\"Starting to match regions for data in {year}/{month}\")\n",
    "        df = addRegions(\n",
    "            df, agRegions\n",
    "        )  # Links data to their crop district number (stored in cr_num)\n",
    "        df = df.reset_index()\n",
    "        print(\n",
    "            f\"Finished matching {len(df.index)} regions for the data in {year}/{month}\"\n",
    "        )\n",
    "\n",
    "        df = addDateAttrs(\n",
    "            df\n",
    "        )  # Breaks down the date attributes into its components and saves them for storage\n",
    "        print(f\"Added date attributes for {year}/{month}\")\n",
    "\n",
    "        print(f\"Adding data from {year}/{month} to the Database\")\n",
    "        df.drop(columns=[\"index\"], inplace=True)\n",
    "        df.to_sql(TABLE, conn, schema=\"public\", if_exists=\"append\", index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\")\n",
    "\n",
    "    # Clean up the environment after the transaction\n",
    "    try:\n",
    "        os.remove(f\"{outputFile}.nc\")\n",
    "        os.remove(f\"{outputFile}.netcdf.zip\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\")\n",
    "\n",
    "    db.cleanup()\n",
    "    print(f\"[SUCCESS] data was pulled for {year}/{month}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa12c0197ba262d3978e6f6673cf27e2255540123d2a48a6d08e62ef766e322e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
