{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pullCopernicusData.ipynb\n",
    "After [loading the agriculture regions](), the following script can be used to load the Copernicus Satellite Data\n",
    "- Only pulls for new dates\n",
    "- Uses logs\n",
    "\n",
    "##### Output:\n",
    "- [agg_day_copernicus_satellite_data](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions#agg_day_copernicus_satellite_data)\n",
    "\n",
    "##### Remarks: null values - na.mask, null etc... can sometimes cause issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CopernicusQueryBuilder import CopernicusQueryBuilder\n",
    "from dotenv import load_dotenv\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "from pandas import Index\n",
    "import geopandas as gpd  # type: ignore\n",
    "import sqlalchemy as sq\n",
    "import xarray as xr  # type: ignore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cdsapi  # type: ignore\n",
    "import os, sys, time, random, zipfile, calendar\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from Shared.DataService import DataService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE = \"agg_day_copernicus_satellite_data\"  # Table name that stores the copernicus satellite data\n",
    "AG_REGIONS_TABLE = \"census_ag_regions\"  # Table name that stores the agriculture regions\n",
    "\n",
    "LOG_FILE = \"data/scrape_copernicus_parallel_20230703.log\"  # The file used to store progress information\n",
    "ERROR_FILE = \"data/scrape_copernicus_parallel_20230703.err\"  # The file used to store error information\n",
    "\n",
    "\n",
    "# Load the database connection environment variables located in the docker folder\n",
    "load_dotenv(\"../docker/.env\")\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "PG_ADDR = os.getenv(\"POSTGRES_ADDR\")\n",
    "PG_PORT = os.getenv(\"POSTGRES_PORT\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PW = os.getenv(\"POSTGRES_PW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 1  # The number of workers we want to employ (maximum is 16 as per the number of cores)\n",
    "REQ_DELAY = 60  # 1 minute - the base delay required to bypass pulling limits\n",
    "MIN_DELAY = 20  # 1 minute - once added to the required delay, creates a minimum delay of 5 minutes to bypass pulling limits\n",
    "MAX_DELAY = 180  # 3 minutes - once added to the required delay, creates a maximum delay of 5 minutes to bypass pulling limits\n",
    "\n",
    "MIN_MONTH = 1  # The month we start pulling data from\n",
    "MAX_MONTH = 12  # The month we stop pulling data from\n",
    "\n",
    "MIN_YEAR = 1995  # The year we start pulling data from\n",
    "MAX_YEAR = 2023  # The year we stop pulling data from\n",
    "\n",
    "# the year range we want to pull data from\n",
    "YEARS = [str(year) for year in range(MIN_YEAR, MAX_YEAR + 1)]\n",
    "\n",
    "# the month range we want to pull data from\n",
    "MONTHS = [str(month) for month in range(MIN_MONTH, MAX_MONTH + 1)]\n",
    "\n",
    "ATTRS = [  # the attributes we want to pull data for\n",
    "    \"2m_dewpoint_temperature\",\n",
    "    \"2m_temperature\",\n",
    "    \"evaporation_from_bare_soil\",\n",
    "    \"skin_reservoir_content\",\n",
    "    \"skin_temperature\",\n",
    "    \"snowmelt\",\n",
    "    \"soil_temperature_level_1\",\n",
    "    \"soil_temperature_level_2\",\n",
    "    \"soil_temperature_level_3\",\n",
    "    \"soil_temperature_level_4\",\n",
    "    \"surface_net_solar_radiation\",\n",
    "    \"surface_pressure\",\n",
    "    \"volumetric_soil_water_layer_1\",\n",
    "    \"volumetric_soil_water_layer_2\",\n",
    "    \"volumetric_soil_water_layer_3\",\n",
    "    \"volumetric_soil_water_layer_4\",\n",
    "    \"leaf_area_index_high_vegetation\",\n",
    "    \"leaf_area_index_low_vegetation\",\n",
    "]\n",
    "\n",
    "HOURS = [  # the hours we want to pull data for\n",
    "    \"00:00\",\n",
    "    \"01:00\",\n",
    "    \"02:00\",\n",
    "    \"03:00\",\n",
    "    \"04:00\",\n",
    "    \"05:00\",\n",
    "    \"06:00\",\n",
    "    \"07:00\",\n",
    "    \"08:00\",\n",
    "    \"09:00\",\n",
    "    \"10:00\",\n",
    "    \"11:00\",\n",
    "    \"12:00\",\n",
    "    \"13:00\",\n",
    "    \"14:00\",\n",
    "    \"15:00\",\n",
    "    \"16:00\",\n",
    "    \"17:00\",\n",
    "    \"18:00\",\n",
    "    \"19:00\",\n",
    "    \"20:00\",\n",
    "    \"21:00\",\n",
    "    \"22:00\",\n",
    "    \"23:00\",\n",
    "]\n",
    "\n",
    "# The area we want data for (specified in EPSG:4326 coordinates)\n",
    "AREA = [\n",
    "    61,\n",
    "    -125,\n",
    "    48,\n",
    "    -88,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if (\n",
    "        PG_DB is None\n",
    "        or PG_ADDR is None\n",
    "        or PG_PORT is None\n",
    "        or PG_USER is None\n",
    "        or PG_PW is None\n",
    "    ):\n",
    "        raise ValueError(\"Environment variables not set\")\n",
    "\n",
    "    # Handles connections to the database\n",
    "    db = DataService(PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW)\n",
    "    queryHandler = CopernicusQueryBuilder()\n",
    "    jobArgs = []  # Holds tuples of arguments for pooled workers\n",
    "\n",
    "    conn = db.connect()  # Connect to the database\n",
    "    queryHandler.createCopernicusTableReq(db)\n",
    "\n",
    "    # Load the agriculture region geometries from the database\n",
    "    agRegions = loadGeometry(conn)\n",
    "\n",
    "    completedDf = getCompleteDates(conn)\n",
    "    db.cleanup()  # Disconnect from the database (workers maintain their own connections)\n",
    "\n",
    "    # Creates the list of arguments (stored as tuples) used in the multiple processes for pullSateliteData(agRegions, year, month, days, outputFile)\n",
    "    for year in YEARS:\n",
    "        for month in MONTHS:\n",
    "            # Calculates the number of days - stored in index 1 of a tuple\n",
    "            numDays = calendar.monthrange(int(year), int(month))[1]\n",
    "\n",
    "            # calculates a random delay (asc for groups of 12)\n",
    "            delay = random.randint(MIN_DELAY, MAX_DELAY)\n",
    "\n",
    "            days = [str(day) for day in range(1, numDays + 1)]\n",
    "            outputFile = f\"data/copernicus_{year}_{month}\"\n",
    "\n",
    "            # removes the days that have already been completed\n",
    "            completedDaysDf = completedDf.loc[\n",
    "                (completedDf[\"year\"] == int(year))\n",
    "                & (completedDf[\"month\"] == int(month))\n",
    "            ]\n",
    "            completedDays = completedDaysDf[\"day\"].tolist()\n",
    "            incompleteDays = []\n",
    "            for day in days:\n",
    "                if int(day) not in completedDays:\n",
    "                    incompleteDays.append(day)\n",
    "\n",
    "            if len(incompleteDays) > 0:\n",
    "                jobArgs.append(\n",
    "                    tuple((agRegions, delay, year, month, incompleteDays, outputFile))\n",
    "                )\n",
    "\n",
    "    # Handles the multiple processes\n",
    "    pool = mp.Pool(NUM_WORKERS)  # Defines the number of workers\n",
    "\n",
    "    # Creates the queue of jobs - pullSateliteData is the function and jobArgs holds the arguments\n",
    "    pool.starmap(pullSatelliteData, jobArgs)\n",
    "    pool.close()  # Once these jobs are finished close the multiple processes pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Outputs progress updates to log files and to the console\n",
    "\n",
    "Pseudocode:  \n",
    "- Check if a filename is provided\n",
    "- [Get the time](https://www.geeksforgeeks.org/python-strftime-function/)\n",
    "- Opens the file and adds the progress message\n",
    "- Print the message to the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateLog(fileName: str, message: str):\n",
    "    if fileName is not None:\n",
    "        with open(fileName, \"a\") as log:\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            log.write(f\"{timestamp} {message}\")\n",
    "\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Load the [regions](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions#census_ag_regions) from the database\n",
    "\n",
    "Pseudocode:  \n",
    "- Create the region SQL query\n",
    "- [Load the regions directly into a GeoDataFrame](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.from_postgis.html)\n",
    "    - crs sets the coordinate system, in our case we want EPSG:3347\n",
    "    - geom_col specifies which column holds the geometry/boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGeometry(conn: sq.engine.Connection) -> gpd.GeoDataFrame:\n",
    "    query = sq.text(f\"select cr_num, district, geometry FROM public.{AG_REGIONS_TABLE}\")\n",
    "\n",
    "    agRegions = gpd.GeoDataFrame.from_postgis(\n",
    "        query, conn, crs=\"EPSG:3347\", geom_col=\"geometry\"\n",
    "    )\n",
    "\n",
    "    return agRegions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Checks which days Copernicus data has already been pulled for\n",
    "\n",
    "Tables:\n",
    "- [agg_day_copernicus_satellite_data](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions#agg_day_copernicus_satellite_data)\n",
    "\n",
    "Pseudocode:  \n",
    "- Create the SQL query to load the dates which are already present in the database\n",
    "- [Load the dates directly into a DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html)\n",
    "\n",
    "Remarks: dates are returned as a DataFrame with year, month and day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCompleteDates(conn: sq.engine.Connection) -> pd.DataFrame:\n",
    "    query = sq.text(f\"select distinct year, month, day from {TABLE}\")\n",
    "    dates = pd.read_sql(query, conn)\n",
    "\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Breaks down the date row into multiple attributes that are easier to work with (year, month and date)\n",
    "\n",
    "Pseudocode:  \n",
    "- For each row, get the date and extract the year, month and date\n",
    "- [Cast these attributes](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) (year, month and date) to an integer\n",
    "- [Delete](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the original datetime row\n",
    "- If an error is encountered, log it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDateAttrs(df: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    try:\n",
    "        for index in range(len(df.index)):\n",
    "            date = pd.Timestamp(np.datetime64(df.at[index, \"datetime\"]))\n",
    "            df.at[index, \"year\"] = date.year\n",
    "            df.at[index, \"month\"] = date.month\n",
    "            df.at[index, \"day\"] = date.day\n",
    "\n",
    "        df[[\"year\", \"month\", \"day\"]] = df[[\"year\", \"month\", \"day\"]].astype(int)\n",
    "        df.drop(columns=[\"datetime\"], inplace=True)\n",
    "    except Exception as e:\n",
    "        updateLog(ERROR_FILE, f\"Error adding date attributes {e}\\n\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Transforms the Copernicus DataFrame into a GeoDataFrame\n",
    "\n",
    "Psuedocode:  \n",
    "- [Create geometry for each set of longitude/latitude](https://geopandas.org/en/stable/docs/reference/api/geopandas.points_from_xy.html) for the data found in the Copernicus data\n",
    "- [Set the coordinate system](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.to_crs.html) to the same one used throughout the codebase (EPSG:3347)\n",
    "- Label the data to the regions [by joining them together](https://geopandas.org/en/stable/docs/reference/api/geopandas.sjoin.html)\n",
    "    - how=left specifies that the Copernicus data is always kept even if it does not fall within a region\n",
    "    - predicate=within joins the data based on which rows of Copernicus data fall into what regions\n",
    "- [Drop irrelevant columns](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)\n",
    "- [Drop irregular data](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.notna.html)\n",
    "- [Cast cr_num and district to integers](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addRegions(df: pd.DataFrame, agRegions: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    # Creates geometry from df using lon and lat as cords to create points (points being geometry)\n",
    "    df = gpd.GeoDataFrame(\n",
    "        df, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(df.lon, df.lat)\n",
    "    )\n",
    "\n",
    "    # Changes the points projection to match the agriculture regions of EPSG:3347\n",
    "    df.to_crs(crs=\"EPSG:3347\", inplace=True)  # type: ignore\n",
    "\n",
    "    # Join the two dataframes based on which points fit within what agriculture regions\n",
    "    df = gpd.sjoin(df, agRegions, how=\"left\", predicate=\"within\")\n",
    "\n",
    "    df.drop(columns=[\"geometry\", \"index_right\", \"lat\", \"lon\"], inplace=True)\n",
    "    df = df[df[\"cr_num\"].notna()]  # Take rows that are valid numbers\n",
    "    df[[\"cr_num\"]] = df[[\"cr_num\"]].astype(int)\n",
    "    df[[\"district\"]] = df[[\"district\"]].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Unzips a zip file and extracts all of its content\n",
    "\n",
    "Pseudocode:  \n",
    "- Open the zipfile (naming convention comes from the Copernicus data request)\n",
    "- Load its contents (*we only expect one*)\n",
    "- Rename the file \n",
    "- [Extract the file](https://www.geeksforgeeks.org/working-zip-files-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipFile(file: str):\n",
    "    with zipfile.ZipFile(f\"./{file}.netcdf.zip\", \"r\") as zip_ref:  # Opens the zip file\n",
    "        zipinfos = zip_ref.infolist()\n",
    "\n",
    "        for zipinfo in zipinfos:  # we only expect one\n",
    "            zipinfo.filename = f\"{file}.nc\"  # Changes the unzipped files name\n",
    "            zip_ref.extract(zipinfo)  # Unzips the file\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Reads a netCEF file\n",
    "\n",
    "Pseudocode:  \n",
    "- [Open the file](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html)\n",
    "- [Convert its contents into a DataFrame](https://docs.xarray.dev/en/latest/generated/xarray.DataArray.to_dataframe.html)\n",
    "- Close the file\n",
    "- If an error is encountered, log it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNetCDF(file: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        dataset = xr.open_dataset(f\"./{file}.nc\")\n",
    "        df = dataset.to_dataframe().reset_index()\n",
    "        dataset.close()\n",
    "    except Exception as e:\n",
    "        updateLog(ERROR_FILE, f\"Error reading netCDF file {e}\\n\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Prepares the DataFrame for future processing\n",
    "\n",
    "Preprocessing:\n",
    "- Add empty attribute columns we will fill later (year, month and day)\n",
    "- [Rename DataFrame columns](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)\n",
    "- Double checks all attributes are present\n",
    "- [Casts all attributes to floats](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html)\n",
    "- Log any errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatDF(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        # Adds the remaining attributes we want to store which will be gathered during data preprocessing\n",
    "        df[\"year\"] = None\n",
    "        df[\"month\"] = None\n",
    "        df[\"day\"] = None\n",
    "\n",
    "        # Renames the dataframes columns so it can be matched when its posted to the database\n",
    "        df.rename(columns={\"longitude\": \"lon\"}, inplace=True)\n",
    "        df.rename(columns={\"latitude\": \"lat\"}, inplace=True)\n",
    "        df.rename(columns={\"time\": \"datetime\"}, inplace=True)\n",
    "        df.rename(columns={\"d2m\": \"dewpoint_temperature\"}, inplace=True)\n",
    "        df.rename(columns={\"t2m\": \"temperature\"}, inplace=True)\n",
    "        df.rename(columns={\"evabs\": \"evaporation_from_bare_soil\"}, inplace=True)\n",
    "        df.rename(columns={\"src\": \"skin_reservoir_content\"}, inplace=True)\n",
    "        df.rename(columns={\"skt\": \"skin_temperature\"}, inplace=True)\n",
    "        df.rename(columns={\"smlt\": \"snowmelt\"}, inplace=True)\n",
    "        df.rename(columns={\"stl1\": \"soil_temperature_level_1\"}, inplace=True)\n",
    "        df.rename(columns={\"stl2\": \"soil_temperature_level_2\"}, inplace=True)\n",
    "        df.rename(columns={\"stl3\": \"soil_temperature_level_3\"}, inplace=True)\n",
    "        df.rename(columns={\"stl4\": \"soil_temperature_level_4\"}, inplace=True)\n",
    "        df.rename(columns={\"ssr\": \"surface_net_solar_radiation\"}, inplace=True)\n",
    "        df.rename(columns={\"sp\": \"surface_pressure\"}, inplace=True)\n",
    "        df.rename(columns={\"swvl1\": \"volumetric_soil_water_layer_1\"}, inplace=True)\n",
    "        df.rename(columns={\"swvl2\": \"volumetric_soil_water_layer_2\"}, inplace=True)\n",
    "        df.rename(columns={\"swvl3\": \"volumetric_soil_water_layer_3\"}, inplace=True)\n",
    "        df.rename(columns={\"swvl4\": \"volumetric_soil_water_layer_4\"}, inplace=True)\n",
    "        df.rename(columns={\"lai_hv\": \"leaf_area_index_high_vegetation\"}, inplace=True)\n",
    "        df.rename(columns={\"lai_lv\": \"leaf_area_index_low_vegetation\"}, inplace=True)\n",
    "\n",
    "        # if column is not in the dataframe then add it with a null value\n",
    "        columns = [\n",
    "            \"lon\",\n",
    "            \"lat\",\n",
    "            \"dewpoint_temperature\",\n",
    "            \"temperature\",\n",
    "            \"evaporation_from_bare_soil\",\n",
    "            \"skin_reservoir_content\",\n",
    "            \"skin_temperature\",\n",
    "            \"snowmelt\",\n",
    "            \"soil_temperature_level_1\",\n",
    "            \"soil_temperature_level_2\",\n",
    "            \"soil_temperature_level_3\",\n",
    "            \"soil_temperature_level_4\",\n",
    "            \"surface_net_solar_radiation\",\n",
    "            \"surface_pressure\",\n",
    "            \"volumetric_soil_water_layer_1\",\n",
    "            \"volumetric_soil_water_layer_2\",\n",
    "            \"volumetric_soil_water_layer_3\",\n",
    "            \"volumetric_soil_water_layer_4\",\n",
    "            \"leaf_area_index_high_vegetation\",\n",
    "            \"leaf_area_index_low_vegetation\",\n",
    "        ]\n",
    "\n",
    "        for column in columns:\n",
    "            if column not in df.columns:\n",
    "                df[column] = None\n",
    "\n",
    "        df[columns] = df[columns].astype(float)\n",
    "    except Exception as e:\n",
    "        updateLog(ERROR_FILE, f\"Error formatting dataframe {e}\\n\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Aggregate the Copernicus Satellite data by year, month, day, cr_num and district\n",
    "\n",
    "Psuedocode:  \n",
    "- [Aggregate](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html) the columns by [year, month, day, cr_num and district](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)\n",
    "- Load all columns\n",
    "- [Realign labels and attribute](https://pandas.pydata.org/docs/reference/api/pandas.Index.html)\n",
    "- [Cast all attributes as floats](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html)\n",
    "- If an error is encountered, log it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDailyAggregate(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        aggregate = (\n",
    "            df.groupby([\"year\", \"month\", \"day\", \"cr_num\", \"district\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"dewpoint_temperature\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"temperature\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"evaporation_from_bare_soil\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"skin_reservoir_content\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"skin_temperature\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"snowmelt\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"soil_temperature_level_1\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"soil_temperature_level_2\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"soil_temperature_level_3\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"soil_temperature_level_4\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"surface_net_solar_radiation\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"surface_pressure\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"volumetric_soil_water_layer_1\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"volumetric_soil_water_layer_2\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"volumetric_soil_water_layer_3\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"volumetric_soil_water_layer_4\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"leaf_area_index_high_vegetation\": [\"min\", \"max\", \"mean\"],\n",
    "                    \"leaf_area_index_low_vegetation\": [\"min\", \"max\", \"mean\"],\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        columns = [\n",
    "            \"min_dewpoint_temperature\",\n",
    "            \"max_dewpoint_temperature\",\n",
    "            \"mean_dewpoint_temperature\",\n",
    "            \"min_temperature\",\n",
    "            \"max_temperature\",\n",
    "            \"mean_temperature\",\n",
    "            \"min_evaporation_from_bare_soil\",\n",
    "            \"max_evaporation_from_bare_soil\",\n",
    "            \"mean_evaporation_from_bare_soil\",\n",
    "            \"min_skin_reservoir_content\",\n",
    "            \"max_skin_reservoir_content\",\n",
    "            \"mean_skin_reservoir_content\",\n",
    "            \"min_skin_temperature\",\n",
    "            \"max_skin_temperature\",\n",
    "            \"mean_skin_temperature\",\n",
    "            \"min_snowmelt\",\n",
    "            \"max_snowmelt\",\n",
    "            \"mean_snowmelt\",\n",
    "            \"min_soil_temperature_level_1\",\n",
    "            \"max_soil_temperature_level_1\",\n",
    "            \"mean_soil_temperature_level_1\",\n",
    "            \"min_soil_temperature_level_2\",\n",
    "            \"max_soil_temperature_level_2\",\n",
    "            \"mean_soil_temperature_level_2\",\n",
    "            \"min_soil_temperature_level_3\",\n",
    "            \"max_soil_temperature_level_3\",\n",
    "            \"mean_soil_temperature_level_3\",\n",
    "            \"min_soil_temperature_level_4\",\n",
    "            \"max_soil_temperature_level_4\",\n",
    "            \"mean_soil_temperature_level_4\",\n",
    "            \"min_surface_net_solar_radiation\",\n",
    "            \"max_surface_net_solar_radiation\",\n",
    "            \"mean_surface_net_solar_radiation\",\n",
    "            \"min_surface_pressure\",\n",
    "            \"max_surface_pressure\",\n",
    "            \"mean_surface_pressure\",\n",
    "            \"min_volumetric_soil_water_layer_1\",\n",
    "            \"max_volumetric_soil_water_layer_1\",\n",
    "            \"mean_volumetric_soil_water_layer_1\",\n",
    "            \"min_volumetric_soil_water_layer_2\",\n",
    "            \"max_volumetric_soil_water_layer_2\",\n",
    "            \"mean_volumetric_soil_water_layer_2\",\n",
    "            \"min_volumetric_soil_water_layer_3\",\n",
    "            \"max_volumetric_soil_water_layer_3\",\n",
    "            \"mean_volumetric_soil_water_layer_3\",\n",
    "            \"min_volumetric_soil_water_layer_4\",\n",
    "            \"max_volumetric_soil_water_layer_4\",\n",
    "            \"mean_volumetric_soil_water_layer_4\",\n",
    "            \"min_leaf_area_index_high_vegetation\",\n",
    "            \"max_leaf_area_index_high_vegetation\",\n",
    "            \"mean_leaf_area_index_high_vegetation\",\n",
    "            \"min_leaf_area_index_low_vegetation\",\n",
    "            \"max_leaf_area_index_low_vegetation\",\n",
    "            \"mean_leaf_area_index_low_vegetation\",\n",
    "        ]\n",
    "\n",
    "        all_columns = [\n",
    "            \"year\",\n",
    "            \"month\",\n",
    "            \"day\",\n",
    "            \"cr_num\",\n",
    "            \"district\",\n",
    "        ]\n",
    "\n",
    "        all_columns.extend(columns)\n",
    "        aggregate.columns = Index(all_columns)\n",
    "        aggregate[columns] = aggregate[columns].astype(float)\n",
    "    except Exception as e:\n",
    "        updateLog(ERROR_FILE, f\"Error generating daily aggregate {e}\\n\")\n",
    "\n",
    "    return aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:  \n",
    "Requests data, processes it then stores the Copernicus Satellite data\n",
    "\n",
    "Tables:  \n",
    "- [agg_day_copernicus_satellite_data](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions#agg_day_copernicus_satellite_data)\n",
    "\n",
    "Psuedocode:  \n",
    "- Adds a time delay (prevents being detected as a bot)\n",
    "- Connect to the database\n",
    "- [Calculate the start time](https://www.geeksforgeeks.org/python-strftime-function/)\n",
    "- [Make a data request](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land?tab=form)\n",
    "- Load the data and preprocess it\n",
    "- Add the region label\n",
    "- Break down the date into its components\n",
    "- Aggreate the data\n",
    "- [Store the data](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html)\n",
    "- [Remnove the accumulated files](https://www.geeksforgeeks.org/python-os-remove-method/)\n",
    "- Log progress/errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullSatelliteData(\n",
    "    agRegions: gpd.GeoDataFrame,\n",
    "    delay: int,\n",
    "    year: str,\n",
    "    month: str,\n",
    "    days: list,\n",
    "    outputFile: str,\n",
    "):\n",
    "    if (\n",
    "        PG_DB is None\n",
    "        or PG_ADDR is None\n",
    "        or PG_PORT is None\n",
    "        or PG_USER is None\n",
    "        or PG_PW is None\n",
    "    ):\n",
    "        updateLog(ERROR_FILE, \"Environment variables not set\")\n",
    "        raise ValueError(\"Environment variables not set\")\n",
    "\n",
    "    db = DataService(PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW)\n",
    "    time.sleep(delay)\n",
    "\n",
    "    conn = db.connect()\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    for currDay in days:\n",
    "        currFile = f\"{outputFile}_{currDay}\"\n",
    "        try:\n",
    "            starttime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            updateLog(\n",
    "                LOG_FILE,\n",
    "                f\"{starttime} Starting to pull data for {year}/{month}/{currDay}\\n\",\n",
    "            )\n",
    "            c.retrieve(\n",
    "                \"reanalysis-era5-land\",\n",
    "                {\n",
    "                    \"format\": \"netcdf.zip\",\n",
    "                    \"variable\": ATTRS,\n",
    "                    \"year\": year,\n",
    "                    \"month\": month,\n",
    "                    \"day\": currDay,\n",
    "                    \"time\": HOURS,\n",
    "                    \"area\": AREA,\n",
    "                },\n",
    "                f\"{currFile}.netcdf.zip\",\n",
    "            )\n",
    "\n",
    "            # Unzips the file, renames it to outputFile and then deletes the source .zip file\n",
    "            unzipFile(currFile)\n",
    "\n",
    "            df = readNetCDF(currFile)  # Converts the netcdf content into a dataframe\n",
    "            # Formats the data frame to process null values, add additional attributes and rename columns\n",
    "\n",
    "            df = formatDF(df)\n",
    "\n",
    "            updateLog(\n",
    "                LOG_FILE,\n",
    "                f\"Starting to match regions for data in {year}/{month}/{currDay}\\n\",\n",
    "            )\n",
    "            # Links data to their crop district number (stored in cr_num)\n",
    "            df = addRegions(df, agRegions)\n",
    "            df = df.reset_index()\n",
    "            updateLog(\n",
    "                LOG_FILE,\n",
    "                f\"Finished matching {len(df.index)} regions for data in {year}/{month}/{currDay}\\n\",\n",
    "            )\n",
    "\n",
    "            # Breaks down the date attributes into its components and saves them for storage\n",
    "            df = addDateAttrs(df)\n",
    "            updateLog(LOG_FILE, f\"Added date attributes for {year}/{month}/{currDay}\\n\")\n",
    "\n",
    "            df = generateDailyAggregate(df)\n",
    "\n",
    "            updateLog(\n",
    "                LOG_FILE,\n",
    "                f\"Adding rows {len(df.index)} data from {year}/{month}/{currDay} to the Database\\n\",\n",
    "            )\n",
    "\n",
    "            df.to_sql(TABLE, conn, schema=\"public\", if_exists=\"append\", index=False)\n",
    "            updateLog(\n",
    "                LOG_FILE,\n",
    "                f\"{starttime} Finished data pull from {year}/{month}/{currDay}\\n\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            updateLog(\n",
    "                ERROR_FILE, f\"Error pulling data for {year}/{month}/{currDay} : {e}\\n\"\n",
    "            )\n",
    "\n",
    "        # Clean up the environment after the transaction\n",
    "        try:\n",
    "            os.remove(f\"{currFile}.nc\")\n",
    "            os.remove(f\"{currFile}.netcdf.zip\")\n",
    "        except Exception as e:\n",
    "            updateLog(ERROR_FILE, f\"Error cleaning up {year}/{month}/{currDay} : {e}\\n\")\n",
    "\n",
    "    db.cleanup()\n",
    "    updateLog(LOG_FILE, f\"Finished adding {year}/{month} to the Database\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
