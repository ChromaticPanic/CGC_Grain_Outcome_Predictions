{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when running this notebook or the associated script it is worthwhile loading the output to a file like so: python pullCopernicusData.py > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2056434/4204842856.py:11: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import calendar\n",
    "import multiprocessing as mp\n",
    "import cdsapi  # type: ignore\n",
    "from dotenv import load_dotenv\n",
    "import sqlalchemy as sq\n",
    "import geopandas as gpd  # type: ignore\n",
    "import xarray as xr  # type: ignore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import black\n",
    "import jupyter_black as bl  # type: ignore\n",
    "from datetime import datetime\n",
    "from CopernicusQueryBuilder import CopernicusQueryBuilder\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from Shared.DataService import DataService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LOG_FILE = \"data/scrape_copernicus_parallel.log\"\n",
    "ERROR_FILE = \"data/scrape_copernicus_parallel.err\"\n",
    "\n",
    "bl.load()\n",
    "load_dotenv(\"../docker/.env\")\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "PG_ADDR = os.getenv(\"POSTGRES_ADDR\")\n",
    "PG_PORT = os.getenv(\"POSTGRES_PORT\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PW = os.getenv(\"POSTGRES_PW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 1  # The number of workers we want to employ (maximum is 16 as per the number of cores)\n",
    "REQ_DELAY = 60  # 1 minute - the base delay required to bypass pulling limits\n",
    "MIN_DELAY = 60  # 1 minute - once added to the required delay, creates a minimum delay of 5 minutes to bypass pulling limits\n",
    "MAX_DELAY = 180  # 3 minutes - once added to the required delay, creates a maximum delay of 5 minutes to bypass pulling limits\n",
    "TABLE = \"copernicus_satellite_data\"\n",
    "\n",
    "MIN_MONTH = 1\n",
    "MAX_MONTH = 12\n",
    "\n",
    "MIN_YEAR = 1995\n",
    "MAX_YEAR = 2023\n",
    "\n",
    "years = [\n",
    "    str(year) for year in range(MIN_YEAR, MAX_YEAR + 1)\n",
    "]  # the year range we want to pull data from\n",
    "months = [\n",
    "    str(month) for month in range(MIN_MONTH, MAX_MONTH + 1)\n",
    "]  # the month range we want to pull data from\n",
    "\n",
    "ATTRS = [  # the attributes we want to pull data for\n",
    "    \"2m_dewpoint_temperature\",\n",
    "    \"2m_temperature\",\n",
    "    \"evaporation_from_bare_soil\",\n",
    "    \"skin_reservoir_content\",\n",
    "    \"skin_temperature\",\n",
    "    \"snowmelt\",\n",
    "    \"soil_temperature_level_1\",\n",
    "    \"soil_temperature_level_2\",\n",
    "    \"soil_temperature_level_3\",\n",
    "    \"soil_temperature_level_4\",\n",
    "    \"surface_net_solar_radiation\",\n",
    "    \"surface_pressure\",\n",
    "    \"volumetric_soil_water_layer_1\",\n",
    "    \"volumetric_soil_water_layer_2\",\n",
    "    \"volumetric_soil_water_layer_3\",\n",
    "    \"volumetric_soil_water_layer_4\",\n",
    "    \"leaf_area_index_high_vegetation\",\n",
    "    \"leaf_area_index_low_vegetation\",\n",
    "]\n",
    "\n",
    "HOURS = [\n",
    "    \"00:00\",\n",
    "    \"01:00\",\n",
    "    \"02:00\",\n",
    "    \"03:00\",\n",
    "    \"04:00\",\n",
    "    \"05:00\",\n",
    "    \"06:00\",\n",
    "    \"07:00\",\n",
    "    \"08:00\",\n",
    "    \"09:00\",\n",
    "    \"10:00\",\n",
    "    \"11:00\",\n",
    "    \"12:00\",\n",
    "    \"13:00\",\n",
    "    \"14:00\",\n",
    "    \"15:00\",\n",
    "    \"16:00\",\n",
    "    \"17:00\",\n",
    "    \"18:00\",\n",
    "    \"19:00\",\n",
    "    \"20:00\",\n",
    "    \"21:00\",\n",
    "    \"22:00\",\n",
    "    \"23:00\",\n",
    "]\n",
    "\n",
    "AREA = [61, -125, 48, -88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateLog(fileName: str, message: str) -> None:\n",
    "    if fileName is not None:\n",
    "        with open(fileName, \"a\") as log:\n",
    "            log.write(message + \"\\n\")\n",
    "            log.close()\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the agriculture regions from the datbase (projection is EPSG:3347)\n",
    "def loadGeometry(conn: sq.engine.Connection) -> gpd.GeoDataFrame:\n",
    "    query = sq.text(\"select cr_num, geometry FROM public.census_ag_regions\")\n",
    "    agRegions = gpd.GeoDataFrame.from_postgis(\n",
    "        query, conn, crs=\"EPSG:3347\", geom_col=\"geometry\"\n",
    "    )\n",
    "\n",
    "    return agRegions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDateAttrs(df: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    try:\n",
    "        for index in range(len(df.index)):\n",
    "            date = pd.Timestamp(np.datetime64(df.at[index, \"datetime\"]))\n",
    "            df.at[index, \"year\"] = date.year\n",
    "            df.at[index, \"month\"] = date.month\n",
    "            df.at[index, \"day\"] = date.day\n",
    "            df.at[index, \"hour\"] = date.hour\n",
    "\n",
    "        df[[\"year\", \"month\", \"day\", \"hour\"]] = df[\n",
    "            [\"year\", \"month\", \"day\", \"hour\"]\n",
    "        ].astype(int)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addRegions(df: pd.DataFrame, agRegions: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    # Creates geometry from df using lon and lat as cords to create points (points being geometry)\n",
    "    df = gpd.GeoDataFrame(\n",
    "        df, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(df.lon, df.lat)\n",
    "    )\n",
    "\n",
    "    # Changes the points projection to match the agriculture regions of EPSG:3347\n",
    "    df.to_crs(crs=\"EPSG:3347\", inplace=True)  # type: ignore\n",
    "\n",
    "    # Join the two dataframes based on which points fit within what agriculture regions\n",
    "    df = gpd.sjoin(df, agRegions, how=\"left\", predicate=\"within\")\n",
    "\n",
    "    df.drop(columns=[\"geometry\", \"index_right\"], inplace=True)\n",
    "    df = df[df[\"cr_num\"].notna()]  # Take rows that are valid numbers\n",
    "    df[[\"cr_num\"]] = df[[\"cr_num\"]].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipFile(file: str):\n",
    "    with zipfile.ZipFile(f\"./{file}.netcdf.zip\", \"r\") as zip_ref:  # Opens the zip file\n",
    "        # Collects the information of each file contained within\n",
    "        zipinfos = zip_ref.infolist()\n",
    "\n",
    "        for zipinfo in zipinfos:  # For each file in the zip file (we only expect one)\n",
    "            zipinfo.filename = f\"{file}.nc\"  # Changes the unzipped files name (once its unzipped of course)\n",
    "            zip_ref.extract(zipinfo)  # Unzips the file\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNetCDF(file: str) -> pd.DataFrame:\n",
    "    dataset = xr.open_dataset(f\"./{file}.nc\")  # Loads the dataset from the netcdf file\n",
    "    # Converts the contents into a dataframe and corrects indexes\n",
    "    df = dataset.to_dataframe().reset_index()\n",
    "\n",
    "    dataset.close()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatDF(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Adds the remaining attributes we want to store which will be gathered during data preprocessing\n",
    "    df[\"year\"] = None\n",
    "    df[\"month\"] = None\n",
    "    df[\"day\"] = None\n",
    "    df[\"hour\"] = None\n",
    "\n",
    "    # Renames the dataframes columns so it can be matched when its posted to the database\n",
    "    df.rename(columns={df.columns[0]: \"lon\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[1]: \"lat\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[2]: \"datetime\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[3]: \"dewpoint_temperature\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[4]: \"temperature\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[5]: \"evaporation_from_bare_soil\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[6]: \"skin_reservoir_content\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[7]: \"skin_temperature\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[8]: \"snowmelt\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[9]: \"soil_temperature_level_1\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[10]: \"soil_temperature_level_2\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[11]: \"soil_temperature_level_3\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[12]: \"soil_temperature_level_4\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[13]: \"surface_net_solar_radiation\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[14]: \"surface_pressure\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[15]: \"volumetric_soil_water_layer_1\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[16]: \"volumetric_soil_water_layer_2\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[17]: \"volumetric_soil_water_layer_3\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[18]: \"volumetric_soil_water_layer_4\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[19]: \"leaf_area_index_high_vegetation\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[20]: \"leaf_area_index_low_vegetation\"}, inplace=True)\n",
    "\n",
    "    # Used to detect null values - na.mask, null etc... will be replaced with nan which get removed immediately after\n",
    "    df[\n",
    "        [\n",
    "            \"lon\",\n",
    "            \"lat\",\n",
    "            \"dewpoint_temperature\",\n",
    "            \"temperature\",\n",
    "            \"evaporation_from_bare_soil\",\n",
    "            \"skin_reservoir_content\",\n",
    "            \"skin_temperature\",\n",
    "            \"snowmelt\",\n",
    "            \"soil_temperature_level_1\",\n",
    "            \"soil_temperature_level_2\",\n",
    "            \"soil_temperature_level_3\",\n",
    "            \"soil_temperature_level_4\",\n",
    "            \"surface_net_solar_radiation\",\n",
    "            \"surface_pressure\",\n",
    "            \"volumetric_soil_water_layer_1\",\n",
    "            \"volumetric_soil_water_layer_2\",\n",
    "            \"volumetric_soil_water_layer_3\",\n",
    "            \"volumetric_soil_water_layer_4\",\n",
    "            \"leaf_area_index_high_vegetation\",\n",
    "            \"leaf_area_index_low_vegetation\",\n",
    "        ]\n",
    "    ] = df[\n",
    "        [\n",
    "            \"lon\",\n",
    "            \"lat\",\n",
    "            \"dewpoint_temperature\",\n",
    "            \"temperature\",\n",
    "            \"evaporation_from_bare_soil\",\n",
    "            \"skin_reservoir_content\",\n",
    "            \"skin_temperature\",\n",
    "            \"snowmelt\",\n",
    "            \"soil_temperature_level_1\",\n",
    "            \"soil_temperature_level_2\",\n",
    "            \"soil_temperature_level_3\",\n",
    "            \"soil_temperature_level_4\",\n",
    "            \"surface_net_solar_radiation\",\n",
    "            \"surface_pressure\",\n",
    "            \"volumetric_soil_water_layer_1\",\n",
    "            \"volumetric_soil_water_layer_2\",\n",
    "            \"volumetric_soil_water_layer_3\",\n",
    "            \"volumetric_soil_water_layer_4\",\n",
    "            \"leaf_area_index_high_vegetation\",\n",
    "            \"leaf_area_index_low_vegetation\",\n",
    "        ]\n",
    "    ].astype(\n",
    "        float\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullSatelliteData(\n",
    "    agRegions: gpd.GeoDataFrame,\n",
    "    delay: int,\n",
    "    year: str,\n",
    "    month: str,\n",
    "    days: list,\n",
    "    outputFile: str,\n",
    "):\n",
    "    if (\n",
    "        PG_DB is None\n",
    "        or PG_ADDR is None\n",
    "        or PG_PORT is None\n",
    "        or PG_USER is None\n",
    "        or PG_PW is None\n",
    "    ):\n",
    "        updateLog(ERROR_FILE, \"Environment variables not set\")\n",
    "        raise ValueError(\"Environment variables not set\")\n",
    "\n",
    "    db = DataService(PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW)\n",
    "    time.sleep(delay)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    updateLog(LOG_FILE, f\"{timestamp}Starting to pull data for {year}/{month}\")\n",
    "    conn = db.connect()\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    try:\n",
    "        c.retrieve(\n",
    "            \"reanalysis-era5-land\",\n",
    "            {\n",
    "                \"format\": \"netcdf.zip\",\n",
    "                \"variable\": ATTRS,\n",
    "                \"year\": year,\n",
    "                \"month\": month,\n",
    "                \"day\": days,\n",
    "                \"time\": HOURS,\n",
    "                \"area\": AREA,\n",
    "            },\n",
    "            f\"{outputFile}.netcdf.zip\",\n",
    "        )\n",
    "\n",
    "        # Unzips the file, renames it to outputFile and then deletes the source .zip file\n",
    "        unzipFile(outputFile)\n",
    "\n",
    "        df = readNetCDF(outputFile)  # Converts the netcdf content into a dataframe\n",
    "        # Formats the data frame to process null values, add additional attributes and rename columns\n",
    "\n",
    "        df = formatDF(df)\n",
    "\n",
    "        updateLog(LOG_FILE, f\"Starting to match regions for data in {year}/{month}\")\n",
    "        # Links data to their crop district number (stored in cr_num)\n",
    "        df = addRegions(df, agRegions)\n",
    "        df = df.reset_index()\n",
    "        updateLog(\n",
    "            LOG_FILE,\n",
    "            f\"Finished matching {len(df.index)} regions for data in {year}/{month}\",\n",
    "        )\n",
    "\n",
    "        # Breaks down the date attributes into its components and saves them for storage\n",
    "        df = addDateAttrs(df)\n",
    "        updateLog(LOG_FILE, f\"Added date attributes for {year}/{month}\")\n",
    "\n",
    "        updateLog(LOG_FILE, f\"Adding data from {year}/{month} to the Database\")\n",
    "        df.drop(columns=[\"index\"], inplace=True)\n",
    "        df.to_sql(TABLE, conn, schema=\"public\", if_exists=\"append\", index=False)\n",
    "    except Exception as e:\n",
    "        updateLog(ERROR_FILE, f\"Error pulling data for {year}/{month} : {e}\")\n",
    "\n",
    "    # Clean up the environment after the transaction\n",
    "    try:\n",
    "        os.remove(f\"{outputFile}.nc\")\n",
    "        os.remove(f\"{outputFile}.netcdf.zip\")\n",
    "    except Exception as e:\n",
    "        updateLog(ERROR_FILE, f\"Error cleaning up {year}/{month} : {e}\")\n",
    "\n",
    "    db.cleanup()\n",
    "    updateLog(LOG_FILE, f\"Finished adding data from {year}/{month} to the Database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if (\n",
    "        PG_DB is None\n",
    "        or PG_ADDR is None\n",
    "        or PG_PORT is None\n",
    "        or PG_USER is None\n",
    "        or PG_PW is None\n",
    "    ):\n",
    "        raise ValueError(\"Environment variables not set\")\n",
    "\n",
    "    # Handles connections to the database\n",
    "    db = DataService(PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW)\n",
    "    queryHandler = CopernicusQueryBuilder()\n",
    "    jobArgs = []  # Holds tuples of arguments for pooled workers\n",
    "    count = 1  # An incrementer used to create unique file names\n",
    "\n",
    "    conn = db.connect()  # Connect to the database\n",
    "    queryHandler.createCopernicusTableReq(db)\n",
    "\n",
    "    # Load the agriculture region geometries from the database\n",
    "    agRegions = loadGeometry(conn)\n",
    "    db.cleanup()  # Disconnect from the database (workers maintain their own connections)\n",
    "\n",
    "    # Creates the list of arguments (stored as tuples) used in the multiple processes for pullSateliteData(agRegions, year, month, days, outputFile)\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            # Calculates the number of days - stored in index 1 of a tuple\n",
    "            numDays = calendar.monthrange(int(year), int(month))[1]\n",
    "\n",
    "            # calculates a random delay (asc for groups of 12)\n",
    "            delay = (count % NUM_WORKERS != 0) * (\n",
    "                REQ_DELAY * (count % NUM_WORKERS) + random.randint(MIN_DELAY, MAX_DELAY)\n",
    "            )\n",
    "\n",
    "            days = [str(day) for day in range(1, numDays + 1)]\n",
    "            outputFile = f\"copernicus{count}\"\n",
    "            count += 1\n",
    "\n",
    "            jobArgs.append(tuple((agRegions, delay, year, month, days, outputFile)))\n",
    "\n",
    "    # Handles the multiple processes\n",
    "    pool = mp.Pool(NUM_WORKERS)  # Defines the number of workers\n",
    "\n",
    "    # Creates the queue of jobs - pullSateliteData is the function and jobArgs holds the arguments\n",
    "    pool.starmap(pullSatelliteData, jobArgs)\n",
    "    pool.close()  # Once these jobs are finished close the multiple processes pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-27 01:39:19Starting to pull data for 1995/1\n",
      "2023-06-27 01:39:19Starting to pull data for 2002/4\n",
      "2023-06-27 01:39:19Starting to pull data for 2009/7\n",
      "2023-06-27 01:39:19Starting to pull data for 2016/10\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Missing/incomplete configuration file: /root/.cdsapirc",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/tmp/ipykernel_2056434/1779461462.py\", line 25, in pullSatelliteData\n    c = cdsapi.Client()\n  File \"/usr/local/lib/python3.8/dist-packages/cdsapi/api.py\", line 304, in __init__\n    raise Exception(\"Missing/incomplete configuration file: %s\" % (dotrc))\nException: Missing/incomplete configuration file: /root/.cdsapirc\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[12], line 45\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m pool \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mPool(NUM_WORKERS)  \u001b[39m# Defines the number of workers\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# Creates the queue of jobs - pullSateliteData is the function and jobArgs holds the arguments\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m pool\u001b[39m.\u001b[39;49mstarmap(pullSatelliteData, jobArgs)\n\u001b[1;32m     46\u001b[0m pool\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:372\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstarmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    367\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, starmapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mException\u001b[0m: Missing/incomplete configuration file: /root/.cdsapirc"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
