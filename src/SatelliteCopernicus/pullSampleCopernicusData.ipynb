{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install cdsapi  \n",
    "\n",
    "This script is helpful in the sense that it helps you understand what data is available and the transformations that occur at each step - note this data should not actually be imported into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, random, zipfile, calendar\n",
    "import cdsapi  # type: ignore\n",
    "from dotenv import load_dotenv\n",
    "import sqlalchemy as sq\n",
    "import geopandas as gpd  # type: ignore\n",
    "import xarray as xr  # type: ignore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from Shared.DataService import DataService  # type: ignore\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "PG_ADDR = os.getenv(\"POSTGRES_ADDR\")\n",
    "PG_PORT = os.getenv(\"POSTGRES_PORT\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PW = os.getenv(\"POSTGRES_PW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 12\n",
    "REQ_DELAY = 120  # the base delay required to bypass pulling limits\n",
    "MIN_DELAY = 60  # 1 minute - once added to the required delay, creates a minimum delay of 5 minutes to bypass pulling limits\n",
    "MAX_DELAY = 180  # 3 minutes - once added to the required delay, creates a maximum delay of 5 minutes to bypass pulling limits\n",
    "TABLE = \"copernicus_satelite_data\"\n",
    "\n",
    "MIN_MONTH = 1\n",
    "MAX_MONTH = 12\n",
    "\n",
    "MIN_YEAR = 1995\n",
    "MAX_YEAR = 2023\n",
    "\n",
    "years = [\n",
    "    str(year) for year in range(MIN_YEAR, MAX_YEAR + 1)\n",
    "]  # the year range we want to pull data from\n",
    "months = [\n",
    "    str(month) for month in range(MIN_MONTH, MAX_MONTH + 1)\n",
    "]  # the month range we want to pull data from\n",
    "\n",
    "ATTRS = [  # the attributes we want to pull data for\n",
    "    \"2m_dewpoint_temperature\",\n",
    "    \"2m_temperature\",\n",
    "    \"evaporation_from_bare_soil\",\n",
    "    \"skin_reservoir_content\",\n",
    "    \"skin_temperature\",\n",
    "    \"snowmelt\",\n",
    "    \"soil_temperature_level_1\",\n",
    "    \"soil_temperature_level_2\",\n",
    "    \"soil_temperature_level_3\",\n",
    "    \"soil_temperature_level_4\",\n",
    "    \"surface_net_solar_radiation\",\n",
    "    \"surface_pressure\",\n",
    "    \"volumetric_soil_water_layer_1\",\n",
    "    \"volumetric_soil_water_layer_2\",\n",
    "    \"volumetric_soil_water_layer_3\",\n",
    "    \"volumetric_soil_water_layer_4\",\n",
    "    \"leaf_area_index_high_vegetation\",\n",
    "    \"leaf_area_index_low_vegetation\",\n",
    "]\n",
    "\n",
    "HOURS = [\n",
    "    \"04:00\",\n",
    "    \"15:00\",\n",
    "]  # pulls what is typically considered to be the coldest and warmest hours of the day\n",
    "AREA = [53, -115, 52, -114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    PG_DB is None\n",
    "    or PG_ADDR is None\n",
    "    or PG_PORT is None\n",
    "    or PG_USER is None\n",
    "    or PG_PW is None\n",
    "):\n",
    "    raise ValueError(\"Environment variables not set\")\n",
    "\n",
    "db = DataService(\n",
    "    PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW\n",
    ")  # Handles connections to the database\n",
    "conn = db.connect()  # Connect to the database\n",
    "\n",
    "query = sq.text(\"select cr_num, geometry FROM public.census_ag_regions\")\n",
    "agRegions = gpd.GeoDataFrame.from_postgis(\n",
    "    query, conn, crs=\"EPSG:3347\", geom_col=\"geometry\"\n",
    ")\n",
    "db.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobArgs = []  # Holds tuples of arguments for pooled workers\n",
    "count = 0  # An incrementer used to create unique file names\n",
    "\n",
    "# Creates the list of arguments (stored as tuples) used in the multiple processes for pullSateliteData(agRegions, year, month, days, outputFile)\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        numDays = calendar.monthrange(int(year), int(month))[\n",
    "            1\n",
    "        ]  # Calculates the number of days - stored in index 1 of a tuple\n",
    "        delay = (count % NUM_WORKERS != 0) * (\n",
    "            REQ_DELAY * (count % NUM_WORKERS) + random.randint(MIN_DELAY, MAX_DELAY)\n",
    "        )\n",
    "\n",
    "        days = [str(day) for day in range(1, numDays + 1)]\n",
    "        outputFile = f\"copernicus{count}\"\n",
    "        count += 1\n",
    "\n",
    "        jobArgs.append(tuple((agRegions, delay, year, month, days, outputFile)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agRegions = jobArgs[0][0]\n",
    "delay = jobArgs[0][1]\n",
    "year = jobArgs[0][2]\n",
    "month = jobArgs[0][3]\n",
    "days = jobArgs[0][4]\n",
    "outputFile = jobArgs[0][5]\n",
    "\n",
    "if (\n",
    "    PG_DB is None\n",
    "    or PG_ADDR is None\n",
    "    or PG_PORT is None\n",
    "    or PG_USER is None\n",
    "    or PG_PW is None\n",
    "):\n",
    "    raise ValueError(\"Environment variables not set\")\n",
    "\n",
    "db = DataService(PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW)\n",
    "time.sleep(delay)\n",
    "\n",
    "print(f\"Starting to pull data for {year}/{month}\")\n",
    "conn = db.connect()\n",
    "c = cdsapi.Client()\n",
    "\n",
    "c.retrieve(\n",
    "    \"reanalysis-era5-land\",\n",
    "    {\n",
    "        \"format\": \"netcdf.zip\",\n",
    "        \"variable\": ATTRS,\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"day\": days,\n",
    "        \"time\": HOURS,\n",
    "        \"area\": AREA,\n",
    "    },\n",
    "    f\"{outputFile}.netcdf.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(f\"{outputFile}.netcdf.zip\", \"r\") as zip_ref:  # Opens the zip file\n",
    "    zipinfos = (\n",
    "        zip_ref.infolist()\n",
    "    )  # Collects the information of each file contained within\n",
    "\n",
    "    for zipinfo in zipinfos:  # For each file in the zip file (we only expect one)\n",
    "        zipinfo.filename = (\n",
    "            outputFile  # Changes the unzipped files name (once its unzipped of course)\n",
    "        )\n",
    "        zip_ref.extract(zipinfo)  # Unzips the file\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xr.open_dataset(outputFile)  # Loads the dataset from the netcdf file\n",
    "df = (\n",
    "    dataset.to_dataframe().reset_index()\n",
    ")  # Converts the contents into a dataframe and corrects indexes\n",
    "\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = None\n",
    "df[\"month\"] = None\n",
    "df[\"day\"] = None\n",
    "df[\"hour\"] = None\n",
    "\n",
    "# Renames the dataframes columns so it can be matched when its posted to the database\n",
    "df.rename(columns={df.columns[0]: \"lon\"}, inplace=True)\n",
    "df.rename(columns={df.columns[1]: \"lat\"}, inplace=True)\n",
    "df.rename(columns={df.columns[2]: \"datetime\"}, inplace=True)\n",
    "df.rename(columns={df.columns[3]: \"dewpoint_temperature\"}, inplace=True)\n",
    "df.rename(columns={df.columns[4]: \"temperature\"}, inplace=True)\n",
    "df.rename(columns={df.columns[5]: \"evaporation_from_bare_soil\"}, inplace=True)\n",
    "df.rename(columns={df.columns[6]: \"skin_reservoir_content\"}, inplace=True)\n",
    "df.rename(columns={df.columns[7]: \"skin_temperature\"}, inplace=True)\n",
    "df.rename(columns={df.columns[8]: \"snowmelt\"}, inplace=True)\n",
    "df.rename(columns={df.columns[9]: \"soil_temperature_level_1\"}, inplace=True)\n",
    "df.rename(columns={df.columns[10]: \"soil_temperature_level_2\"}, inplace=True)\n",
    "df.rename(columns={df.columns[11]: \"soil_temperature_level_3\"}, inplace=True)\n",
    "df.rename(columns={df.columns[12]: \"soil_temperature_level_4\"}, inplace=True)\n",
    "df.rename(columns={df.columns[13]: \"surface_net_solar_radiation\"}, inplace=True)\n",
    "df.rename(columns={df.columns[14]: \"surface_pressure\"}, inplace=True)\n",
    "df.rename(columns={df.columns[15]: \"volumetric_soil_water_layer_1\"}, inplace=True)\n",
    "df.rename(columns={df.columns[16]: \"volumetric_soil_water_layer_2\"}, inplace=True)\n",
    "df.rename(columns={df.columns[17]: \"volumetric_soil_water_layer_3\"}, inplace=True)\n",
    "df.rename(columns={df.columns[18]: \"volumetric_soil_water_layer_4\"}, inplace=True)\n",
    "df.rename(columns={df.columns[19]: \"leaf_area_index_high_vegetation\"}, inplace=True)\n",
    "df.rename(columns={df.columns[20]: \"leaf_area_index_low_vegetation\"}, inplace=True)\n",
    "\n",
    "# Used to detect null values - na.mask, null etc... will be replaced with nan which get removed immediately after\n",
    "df[\n",
    "    [\n",
    "        \"lon\",\n",
    "        \"lat\",\n",
    "        \"dewpoint_temperature\",\n",
    "        \"temperature\",\n",
    "        \"evaporation_from_bare_soil\",\n",
    "        \"skin_reservoir_content\",\n",
    "        \"skin_temperature\",\n",
    "        \"snowmelt\",\n",
    "        \"soil_temperature_level_1\",\n",
    "        \"soil_temperature_level_2\",\n",
    "        \"soil_temperature_level_3\",\n",
    "        \"soil_temperature_level_4\",\n",
    "        \"surface_net_solar_radiation\",\n",
    "        \"surface_pressure\",\n",
    "        \"volumetric_soil_water_layer_1\",\n",
    "        \"volumetric_soil_water_layer_2\",\n",
    "        \"volumetric_soil_water_layer_3\",\n",
    "        \"volumetric_soil_water_layer_4\",\n",
    "        \"leaf_area_index_high_vegetation\",\n",
    "        \"leaf_area_index_low_vegetation\",\n",
    "    ]\n",
    "] = df[\n",
    "    [\n",
    "        \"lon\",\n",
    "        \"lat\",\n",
    "        \"dewpoint_temperature\",\n",
    "        \"temperature\",\n",
    "        \"evaporation_from_bare_soil\",\n",
    "        \"skin_reservoir_content\",\n",
    "        \"skin_temperature\",\n",
    "        \"snowmelt\",\n",
    "        \"soil_temperature_level_1\",\n",
    "        \"soil_temperature_level_2\",\n",
    "        \"soil_temperature_level_3\",\n",
    "        \"soil_temperature_level_4\",\n",
    "        \"surface_net_solar_radiation\",\n",
    "        \"surface_pressure\",\n",
    "        \"volumetric_soil_water_layer_1\",\n",
    "        \"volumetric_soil_water_layer_2\",\n",
    "        \"volumetric_soil_water_layer_3\",\n",
    "        \"volumetric_soil_water_layer_4\",\n",
    "        \"leaf_area_index_high_vegetation\",\n",
    "        \"leaf_area_index_low_vegetation\",\n",
    "    ]\n",
    "].astype(\n",
    "    float\n",
    ")\n",
    "\n",
    "df = df.replace(np.nan, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.GeoDataFrame(\n",
    "    df, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(df.lon, df.lat)\n",
    ")  # Creates geometry from df using lon and lat as cords to create points (points being geometry)\n",
    "df = df.to_crs(  # type: ignore\n",
    "    crs=\"EPSG:3347\", inplace=True\n",
    ")  # Changes the points projection to match the agriculture regions of EPSG:3347\n",
    "df = gpd.sjoin(\n",
    "    df, agRegions, how=\"inner\", predicate=\"within\"\n",
    ")  # Join the two dataframes based on which points fit within what agriculture regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"geometry\", \"index_right\"], inplace=True)\n",
    "df[[\"cr_num\"]] = df[[\"cr_num\"]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(df.index)):\n",
    "    date = pd.Timestamp(np.datetime64(df.at[index, \"datetime\"]))\n",
    "    df.at[index, \"year\"] = date.year\n",
    "    df.at[index, \"month\"] = date.month\n",
    "    df.at[index, \"day\"] = date.day\n",
    "    df.at[index, \"hour\"] = date.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(TABLE, conn, schema=\"public\", if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[SUCCESS] data was pulled for {year}/{month}\")\n",
    "db.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa12c0197ba262d3978e6f6673cf27e2255540123d2a48a6d08e62ef766e322e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
