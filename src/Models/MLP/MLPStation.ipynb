{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how this notebook works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy as sq\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras_tuner.tuners import RandomSearch, BayesianOptimization\n",
    "\n",
    "from sklearn.metrics import (  # type: ignore\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "os.chdir(\"../../\")\n",
    "from ModelBuilderMethods import getConn\n",
    "from Datasets.DataTestSplit import splitData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.config.set_visible_devices([], 'GPU')  # Hide GPU devices\n",
    "tensorflow.config.set_visible_devices(tensorflow.config.list_physical_devices('CPU'), 'CPU')  # Show CPU devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlimited line output\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", 500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherStationQuery = sq.text(\n",
    "    \"\"\"\n",
    "    SELECT * from dataset_cross_monthly_station\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "weatherSatQuery = sq.text(\n",
    "    \"\"\"\n",
    "    SELECT * from dataset_cross_monthly_sat\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "ergotPrevYearsAggQuery = sq.text(\n",
    "    \"\"\"\n",
    "    SELECT year, district, \n",
    "    present_prev1, present_prev2, present_prev3,\n",
    "    percnt_true_prev1, percnt_true_prev2, percnt_true_prev3 \n",
    "    from agg_ergot_sample_v2\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "ergotTargetQuery = sq.text(\n",
    "    \"\"\"\n",
    "    SELECT year, district, downgrade from ergot_sample_feat_eng\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = getConn(\"./.env\")\n",
    "\n",
    "stationDf = pd.read_sql(weatherStationQuery, conn)\n",
    "# satelliteDf = pd.read_sql(weatherSatQuery, conn)\n",
    "ergotPrevDf = pd.read_sql(ergotPrevYearsAggQuery, conn)\n",
    "ergotTargetDf = pd.read_sql(ergotTargetQuery, conn)\n",
    "\n",
    "conn.close()\n",
    "del conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on year and district\n",
    "# tempdf = pd.merge(satelliteDf, ergotPrevDf, on=[\"year\", \"district\"], how=\"left\")\n",
    "# del satelliteDf\n",
    "# del ergotPrevDf\n",
    "# tempdf = satelliteDf\n",
    "tempdf = stationDf\n",
    "\n",
    "# merge on year and district\n",
    "datasetDf = pd.merge(ergotTargetDf, tempdf, on=[\"year\", \"district\"], how=\"left\")\n",
    "del ergotTargetDf\n",
    "del tempdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categorical values [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode district\n",
    "datasetDf[\"district\"] = datasetDf[\"district\"].astype(\"category\")\n",
    "\n",
    "temp = pd.get_dummies(datasetDf[\"district\"], prefix=\"district\", drop_first=True)\n",
    "datasetDf = pd.concat([datasetDf, temp], axis=1)\n",
    "\n",
    "datasetDf = datasetDf.drop(columns=[\"district\"])\n",
    "\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = [\"year\"]\n",
    "target_variable = \"downgrade\"\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splitData(datasetDf, drop_features, target_variable, 2019,0.2, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balancing the dataset https://imbalanced-learn.org/stable/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre balancing check\n",
    "# print value counts downgrade\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling data\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_rs, y_train_rs = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization / scaling\n",
    "some blurb about scalers  \n",
    "0 [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)             \n",
    "1 [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)  \n",
    "2 [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)  \n",
    "3 [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)  \n",
    "4 [Normalizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html)  \n",
    "5 [PowerTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html)  \n",
    "6 [QuantileTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selecting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    auc = tensorflow.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tensorflow.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this model if you want to see the accuray\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Dense(units=hp.Int('units_input', min_value=32, max_value=256, step=32),\n",
    "                           activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    for i in range(hp.Int('num_layers', min_value=1, max_value=5)):\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
    "                               activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with the desired optimizer, loss, and metrics\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# # use this moel if you want to see the auc\n",
    "# def build_model(hp):\n",
    "#     model = keras.Sequential()\n",
    "#     model.add(Dense(units=hp.Int('units_input', min_value=32, max_value=256, step=32),\n",
    "#                            activation='relu', input_shape=(X_train.shape[1],)))\n",
    "#     for i in range(hp.Int('num_layers', min_value=1, max_value=5)):\n",
    "#         model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
    "#                                activation='relu'))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#     # Compile the model with the desired optimizer, loss, and metrics\n",
    "#     model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "#                   loss='binary_crossentropy', metrics=['accuracy', tensorflow.keras.metrics.AUC(name='auc')])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    # objective=kt.Objective(\"val_auc\", direction=\"max\"),           # if you want to maximize AUC\n",
    "    objective=\"val_accuracy\",                                       # if you want to maximize accuracy\n",
    "    max_trials=10,\n",
    "    overwrite=True,\n",
    "    executions_per_trial=2,\n",
    "    directory='data/BayesianOptimization',\n",
    "    project_name='ergot_random_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 20\n",
    "# run the search\n",
    "tuner.search(X_train_rs, y_train_rs, epochs=EPOCHES, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method : 1\n",
    "# model = tuner.hypermodel.build(best_hps)\n",
    "# model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
    "\n",
    "# Method : 2\n",
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "model.build(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using  validation_data\n",
    "\n",
    "history = model.fit(X_train_rs, y_train_rs, epochs=EPOCHES, batch_size=64, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the training and validation loss for each epoch\n",
    "def evaluate_model(history):\n",
    "    # Get the training and validation loss from the history\n",
    "    training_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    # Get the training and validation accuracy from the history\n",
    "    training_accuracy = history.history['accuracy']\n",
    "    validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    # Plot the training and validation loss\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(training_loss, label='Training Loss')\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot the training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(training_accuracy, label='Training Accuracy')\n",
    "    plt.plot(validation_accuracy, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model: Sequential, X_test: np.ndarray, y_test: np.ndarray):\n",
    "    y_log = model.predict(X_test)\n",
    "    y_pred = np.where(y_log > 0.7, 1, 0)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    accuracy = (conf_matrix[0, 0] + conf_matrix[1, 1]) / np.sum(conf_matrix)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "    precision = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[0, 1])\n",
    "    print(\"Precision: \", precision)\n",
    "\n",
    "    recall = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 0])\n",
    "    print(\"Recall: \", recall)\n",
    "\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    print(\"F1 Score: \", f1_score)\n",
    "\n",
    "    auc_score = roc_auc_score(y_val, y_pred)\n",
    "    print(\"AUC Score: \", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict(model, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
