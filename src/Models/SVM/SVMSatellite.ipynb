{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM For Weather Satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split  # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler  # type: ignore\n",
    "\n",
    "# import dependencies\n",
    "import pandas as pd\n",
    "import sqlalchemy as sq\n",
    "import sys, os\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.svm import SVC  # type: ignore\n",
    "\n",
    "from sklearn.metrics import (  # type: ignore\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "os.chdir(\"../../\")\n",
    "from ModelBuilderMethods import getConn, extractYears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlimited line output\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", 500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**Step 1**</u>: Data Selection\n",
    "\n",
    "In this step, we would choose the particular data/table, pick attributes from existing tables. Further aggregation/feature engineer can be done here to support the point of the research.\n",
    "\n",
    "Particular, for this notebook, we grab the following data and merge them (on year, district) into a single table:\n",
    "- Monthly weather satellite data\n",
    "- ergot data (downgrade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the query text\n",
    "weatherSatQuery = sq.text(\n",
    "    \"\"\"\n",
    "    SELECT * from dataset_cross_monthly_sat\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "ergotTargetQuery = sq.text(\n",
    "    \"\"\"\n",
    "    SELECT year, district, downgrade from ergot_sample_feat_eng\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = getConn(\"./.env\")\n",
    "\n",
    "satelliteDf = pd.read_sql(weatherSatQuery, conn)\n",
    "ergotTargetDf = pd.read_sql(ergotTargetQuery, conn)\n",
    "\n",
    "conn.close()\n",
    "del conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = satelliteDf\n",
    "\n",
    "# merge on year and district\n",
    "datasetDf = pd.merge(ergotTargetDf, tempdf, on=[\"year\", \"district\"], how=\"left\")\n",
    "del ergotTargetDf\n",
    "del tempdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode district\n",
    "datasetDf[\"district\"] = datasetDf[\"district\"].astype(\"category\")\n",
    "\n",
    "temp = pd.get_dummies(datasetDf[\"district\"], prefix=\"district\", drop_first=True)\n",
    "datasetDf = pd.concat([datasetDf, temp], axis=1)\n",
    "\n",
    "datasetDf = datasetDf.drop(columns=[\"district\"])\n",
    "\n",
    "del temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**Step 2**</u>: Splitting dataset\n",
    "\n",
    "- We split the whole dataset into the train/test split. Particularly, split them by year (1995 - 2015 for training, 2016 - 2020 for testing) since this is a time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 1995 - 2015 test 2016 - 2020\n",
    "trainDf = extractYears(datasetDf, 1995, 2015)\n",
    "testDf = extractYears(datasetDf, 2016, 2020)\n",
    "del datasetDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop year\n",
    "trainDf = trainDf.drop(columns=[\"year\"])\n",
    "testDf = testDf.drop(columns=[\"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downgrade\n",
      "False    122202\n",
      "True       2082\n",
      "Name: count, dtype: int64\n",
      "downgrade\n",
      "False    26307\n",
      "True      1016\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# pre balancing check\n",
    "# print value counts downgrade\n",
    "print(trainDf[\"downgrade\"].value_counts())\n",
    "print(testDf[\"downgrade\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downgrade                           0\n",
      "1:min_dewpoint_temperature          0\n",
      "1:min_temperature                   0\n",
      "1:min_evaporation_from_bare_soil    0\n",
      "1:min_skin_reservoir_content        0\n",
      "                                   ..\n",
      "district_4830                       0\n",
      "district_4840                       0\n",
      "district_4850                       0\n",
      "district_4860                       0\n",
      "district_4870                       0\n",
      "Length: 687, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count nan\n",
    "print(trainDf.isna().sum())\n",
    "# set nan to 0\n",
    "trainDf = trainDf.fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**Step 3**</u>: [Balancing the dataset](https://imbalanced-learn.org/stable/)\n",
    "\n",
    "- Our dataset is unbalanced and can lead to bias when training/testing. Balacing step would help to eliminate the bias of the dataset, thus provide more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancer = SMOTEENN(sampling_strategy=1, random_state=42)\n",
    "balancedTrainDfX, balancedTrainDfY = balancer.fit_resample(\n",
    "    trainDf.drop(columns=\"downgrade\"), trainDf[\"downgrade\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downgrade\n",
      "False    115239\n",
      "True      25156\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(balancedTrainDfY.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**Step 4**</u>: Regularization / Normalization\n",
    "some blurb about scalers  \n",
    "\n",
    "1. [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)             \n",
    "2. [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)  \n",
    "3. [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)  \n",
    "4. [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)  \n",
    "5. [Normalizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html)  \n",
    "6. [PowerTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html)  \n",
    "7. [QuantileTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMetrics(model_name, y_true, y_pred):\n",
    "    print(model_name)\n",
    "    print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision: \", precision_score(y_true, y_pred))\n",
    "    print(\"Recall: \", recall_score(y_true, y_pred))\n",
    "    print(\"F1: \", f1_score(y_true, y_pred))\n",
    "    print(\"ROC AUC: \", roc_auc_score(y_true, y_pred))\n",
    "    print(\"Classification Report: \\n\", classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**Step 5**</u>: SVM Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>**Step 5.1**</u>: Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel=\"rbf\", C=1)\n",
    "balanced_svm_model = SVC(kernel=\"rbf\", C=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>**Step 5.2**</u>: Fit the training data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.fit(trainDf.drop(columns=\"downgrade\"), trainDf[\"downgrade\"])\n",
    "balanced_svm_model.fit(balancedTrainDfX, balancedTrainDfY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>**Step 5.3**</u>: Test the model on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svm = svm_model.predict(testDf.drop(columns=\"downgrade\"))\n",
    "predictions_balanced_svm = balanced_svm_model.predict(testDf.drop(columns=\"downgrade\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(predictions_svm).value_counts())\n",
    "print(pd.DataFrame(predictions_balanced_svm).value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>**Step 5.4**</u>: Evaluate models based on different metrics:\n",
    "- ACCURACY:\n",
    "- PRECISION:\n",
    "- RECALL:\n",
    "- F1:\n",
    "- ROC AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(\"SVM imbalanced train set\", testDf[\"downgrade\"], predictions_svm)\n",
    "printMetrics(\n",
    "    \"SVM balanced train set\",\n",
    "    testDf[\"downgrade\"],\n",
    "    predictions_balanced_svm,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>**Step 5.5**</u>: Hypertune the hyperparameters on different kernels, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernals = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "c_list = [0.1, 1, 10, 100]\n",
    "\n",
    "for k in kernals:\n",
    "    for c in c_list:\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"Kernal = {k}, C = {c}\")\n",
    "\n",
    "        # Train & Test model on unbalanced data\n",
    "        unbal_model = SVC(kernel=k, C=c)\n",
    "        unbal_model.fit(trainDf.drop(columns=\"downgrade\"), trainDf[\"downgrade\"])\n",
    "        y_pred = unbal_model.predict(testDf.drop(columns=\"downgrade\"))\n",
    "        printMetrics(\"SVM imbalanced train set\", testDf[\"downgrade\"], unbal_model)\n",
    "\n",
    "        # Train & Test model on balanced data\n",
    "        bal_model = SVC(kernel=k, C=c)\n",
    "        bal_model.fit(trainDf.drop(columns=\"downgrade\"), trainDf[\"downgrade\"])\n",
    "        y_pred = bal_model.predict(testDf.drop(columns=\"downgrade\"))\n",
    "        printMetrics(\n",
    "            \"SVM balanced train set\",\n",
    "            testDf[\"downgrade\"],\n",
    "            bal_model,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
