{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory on Weather Station Data\n",
    "\n",
    "- Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture designed to model sequential data by maintaining a memory of past information while selectively updating and forgetting information over time.\n",
    "\n",
    "- They are particularly effective in capturing long-range dependencies in sequences, making them well-suited for tasks like natural language processing, speech recognition, and time series forecasting. They combat the vanishing gradient problem of traditional RNNs through a sophisticated gating mechanism that enables them to learn and retain information for longer periods.\n",
    "\n",
    "- In this notebook, we grasp all data based on data steps. For example: if k (step) = 100, we take convert each data point in our dataset to (100, 1) means using 100 data points before that time step to predict the next 101 time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sq\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.ensemble import (  # type: ignore\n",
    "    RUSBoostClassifier,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (  # type: ignore\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "os.chdir(\"../../\")\n",
    "from ModelBuilderMethods import getConn, extractYears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlimited line output\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", 500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**Step 1**</u>: Data Selection\n",
    "\n",
    "In this step, we would choose the particular data/table, pick attributes from existing tables. Further aggregation/feature engineer can be done here to support the point of the research.\n",
    "\n",
    "Particular, for this notebook, we grab the following data and merge them (on year, district) into a single table:\n",
    "- Monthly weather station\n",
    "- ergot data (downgrade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the query text\n",
    "weatherStationQuery = sq.text(\n",
    "    \"\"\"\n",
    "    SELECT * from dataset_cross_monthly_station\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "ergotTargetQuery = sq.text(\n",
    "    \"\"\"\n",
    "    SELECT year, district, downgrade from ergot_sample_feat_eng\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = getConn()\n",
    "\n",
    "stationDf = pd.read_sql(weatherStationQuery, conn)\n",
    "ergotTargetDf = pd.read_sql(ergotTargetQuery, conn)\n",
    "\n",
    "conn.close()\n",
    "del conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = stationDf\n",
    "\n",
    "# merge on year and district\n",
    "datasetDf = pd.merge(ergotTargetDf, tempdf, on=[\"year\", \"district\"], how=\"left\")\n",
    "del ergotTargetDf\n",
    "del tempdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode district\n",
    "datasetDf[\"district\"] = datasetDf[\"district\"].astype(\"category\")\n",
    "\n",
    "temp = pd.get_dummies(datasetDf[\"district\"], prefix=\"district\", drop_first=True)\n",
    "datasetDf = pd.concat([datasetDf, temp], axis=1)\n",
    "\n",
    "datasetDf = datasetDf.drop(columns=[\"district\"])\n",
    "\n",
    "del temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**Step 2**</u>: Splitting dataset\n",
    "\n",
    "- We split the whole dataset into the train/test split. Particularly, split them by year (1995 - 2015 for training, 2016 - 2020 for testing) since this is a time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 1995 - 2015 test 2016 - 2020\n",
    "trainDf = extractYears(datasetDf, 1995, 2015)\n",
    "testDf = extractYears(datasetDf, 2016, 2020)\n",
    "del datasetDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop year\n",
    "trainDf = trainDf.drop(columns=[\"year\"])\n",
    "testDf = testDf.drop(columns=[\"year\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**Step 3**</u>: [Balancing the dataset](https://imbalanced-learn.org/stable/)\n",
    "\n",
    "- Our dataset is unbalanced and can lead to bias when training/testing. Balacing step would help to eliminate the bias of the dataset, thus provide more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre balancing check\n",
    "# print value counts downgrade\n",
    "print(trainDf[\"downgrade\"].value_counts())\n",
    "print(testDf[\"downgrade\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nan\n",
    "print(trainDf.isna().sum())\n",
    "# set nan to 0\n",
    "trainDf = trainDf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancer = SMOTEENN(sampling_strategy=1, random_state=42)\n",
    "balancedTrainDfX, balancedTrainDfY = balancer.fit_resample(\n",
    "    trainDf.drop(columns=\"downgrade\"), trainDf[\"downgrade\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post balancing check\n",
    "# print value counts downgrade\n",
    "print(balancedTrainDfY.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**Step 4**</u>: Regularization / Normalization\n",
    "some blurb about scalers  \n",
    "\n",
    "1. [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)             \n",
    "2. [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)  \n",
    "3. [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)  \n",
    "4. [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)  \n",
    "5. [Normalizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html)  \n",
    "6. [PowerTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html)  \n",
    "7. [QuantileTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMetrics(model_name, y_true, y_pred):\n",
    "    print(model_name)\n",
    "    print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision: \", precision_score(y_true, y_pred))\n",
    "    print(\"Recall: \", recall_score(y_true, y_pred))\n",
    "    print(\"F1: \", f1_score(y_true, y_pred))\n",
    "    print(\"ROC AUC: \", roc_auc_score(y_true, y_pred))\n",
    "    print(\"Classification Report: \\n\", classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**Step 5**</u>: Long Short Term Memory Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>**Step 5.0**</u>: Create input-output pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_io_pair(\n",
    "    X_train: np.ndarray, Y_train: np.ndarray, k=100\n",
    ") -> \"tuple(np.ndarray, np.ndarray)\":\n",
    "    \"\"\"\n",
    "    k: time step\n",
    "    return: (input, output) pairs from given data\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    windows_y = []\n",
    "\n",
    "    for i, sequence in enumerate(X_train):\n",
    "        len_seq = len(sequence)\n",
    "        for window_start in range(0, len_seq - k + 1):\n",
    "            window_end = window_start + k\n",
    "            window = sequence[window_start:window_end]\n",
    "            windows.append(window)\n",
    "            windows_y.append(Y_train[i])\n",
    "    return (np.array(windows), np.array(windows_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(trainDf.drop(columns=[\"downgrade\"]))\n",
    "Y_train = np.array(trainDf[\"downgrade\"])\n",
    "x_train, y_train = create_io_pair(X_train, Y_train, k=100)\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(testDf.drop(columns=[\"downgrade\"]))\n",
    "Y_test = np.array(testDf[\"downgrade\"])\n",
    "x_test, y_test = create_io_pair(X_test, Y_test, k=100)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>**Step 5.1**</u>: Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(n_input, n_output, units=50, dropout_rate=0.2, optimizer=\"adam\"):\n",
    "    # using sequential to build LSTM model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adding the first LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units=units, return_sequences=True, input_shape=(n_input, 1)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Adding a second LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units=units, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Adding a third LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units=units, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Adding a fourth LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units=units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Adding the output layer\n",
    "    model.add(Dense(units=n_output))\n",
    "\n",
    "    # Compiling the RNN\n",
    "    model.compile(optimizer=optimizer, loss=\"mean_absolute_error\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_model(\n",
    "    x_train.shape[1], x_train.shape[2], units=50, dropout_rate=0.2, optimizer=\"adam\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>**Step 5.2**</u>: Fit the training data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, epochs=100, batch_size=64\n",
    ")  ## this will be terminated when the input size is too large"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>**Step 5.3**</u>: Test the model on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_bal_LSTM = model.predict(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>**Step 5.4**</u>: Evaluate models based on different metrics:\n",
    "- ACCURACY:\n",
    "- PRECISION:\n",
    "- RECALL:\n",
    "- F1:\n",
    "- ROC AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(\n",
    "    \"LSTM unbalanced train set\",\n",
    "    testDf[\"downgrade\"],\n",
    "    prediction_bal_LSTM,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
