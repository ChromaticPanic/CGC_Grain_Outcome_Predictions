{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aggregateHly.ipynb\n",
    "After loading the [hourly weather stations](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions#stations_hly) and the [hourly weather station data](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions#ab_hly_station_data) the following class can be used to calculate the minimum, mean and maximum of all attributes per district\n",
    "\n",
    "##### Output:\n",
    "An excel document with the expected output columns (saves as specified by pathToSave i.e datasets uses datasets/data/)\n",
    "\n",
    "##### Remarks: \n",
    "- As weeks change per year, the weekly aggregation uses the year of 2001 (not a leap year)\n",
    "- Although a tablename is assigned there is a column limit in postgres that hinders this option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sqlalchemy as sq\n",
    "import geopandas as gpd  # type: ignore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, calendar\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from Shared.DataService import DataService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HLY_STATIONS = \"stations_hly\"  # table that contains the hourly stations\n",
    "\n",
    "AB_HLY_TABLE = \"ab_hly_station_data\"  # table that contains Albertas data\n",
    "MB_HLY_TABLE = \"mb_hly_station_data\"  # table that contains Manitobas data\n",
    "SK_HLY_TABLE = \"sk_hly_station_data\"  # table that contains Saskatchewans data\n",
    "\n",
    "MIN_MONTH = 1  # The month to start aggregating on\n",
    "MAX_MONTH = 12  # The month to finish aggregating on\n",
    "\n",
    "MIN_YEAR = 1995  # The year to start aggregating on\n",
    "MAX_YEAR = 2022  # The year to finish aggregating on\n",
    "TABLENAME = \"agg_hly_weather\"  # Name of the table where results are stored\n",
    "\n",
    "\n",
    "# Load the database connection environment variables located in the docker folder\n",
    "load_dotenv(\"../docker/.env\")\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "PG_ADDR = os.getenv(\"POSTGRES_ADDR\")\n",
    "PG_PORT = os.getenv(\"POSTGRES_PORT\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PW = os.getenv(\"POSTGRES_PW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if (\n",
    "        PG_DB is None\n",
    "        or PG_ADDR is None\n",
    "        or PG_PORT is None\n",
    "        or PG_USER is None\n",
    "        or PG_PW is None\n",
    "    ):\n",
    "        raise ValueError(\"Environment variables not set\")\n",
    "\n",
    "    db = DataService(PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW)\n",
    "    conn = db.connect()\n",
    "\n",
    "    weatherData = pullWeatherData(conn)\n",
    "    stationData = pullStationData(conn)\n",
    "\n",
    "    # merge both the weather station data and the station data together\n",
    "    df = weatherData.merge(stationData, on=\"station_id\")\n",
    "\n",
    "    agg_df = aggregateHlyData(df)\n",
    "    dates = getDates()\n",
    "    listForDF = reshapeHlyData(dates, agg_df, stationData)\n",
    "    final_df = pd.DataFrame(listForDF)\n",
    "\n",
    "    try:\n",
    "        final_df.to_csv(\n",
    "            path_or_buf=\"data/aggregatedHly.csv\",\n",
    "            sep=\",\",\n",
    "            columns=final_df.columns.tolist(),\n",
    "        )\n",
    "        # final_df.to_sql(TABLENAME, conn, schema=\"public\", if_exists=\"append\", index=False)\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR]\")\n",
    "        print(e)\n",
    "\n",
    "    db.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullWeatherData(conn: sq.engine.Connection) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Loads the weather station data per province from the weather station data tables\n",
    "\n",
    "    Tables:\n",
    "    - [ab_hly_station_data](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions#ab_hly_station_data)\n",
    "    - [mb_hly_station_data](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions#mb_hly_station_data)\n",
    "    - [sk_hly_station_data](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions#sk_hly_station_data)\n",
    "\n",
    "    Psuedocode:\n",
    "    - Create the weather station data SQL query\n",
    "    - [Load the data from the database directly into a DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html)\n",
    "    \"\"\"\n",
    "    weatherDataQuery = sq.text(\n",
    "        f\"\"\"\n",
    "        SELECT * FROM public.{AB_HLY_TABLE}\n",
    "        UNION\n",
    "        SELECT * FROM public.{MB_HLY_TABLE}\n",
    "        UNION\n",
    "        SELECT * FROM public.{SK_HLY_TABLE};\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    return pd.read_sql(weatherDataQuery, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullStationData(conn: sq.engine.Connection) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Loads the weather stations from the hourly weather station table\n",
    "\n",
    "    Tables:\n",
    "    - [stations_hly](https://github.com/ChromaticPanic/CGC_Grain_Outcome_Predictions#stations_hly)\n",
    "\n",
    "    Psuedocode:\n",
    "    - Create the weather station SQL query\n",
    "    - [Load the data from the database directly into a DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html)\n",
    "    - [Cast district into an integer](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html)\n",
    "\n",
    "    Remarks: if district is not casted, future methods will throw errors\n",
    "    \"\"\"\n",
    "    stationDataQuery = sq.text(\n",
    "        f\"\"\"\n",
    "        SELECT station_id, district FROM public.{HLY_STATIONS}\n",
    "        WHERE district IS NOT NULL;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    stationData = pd.read_sql(stationDataQuery, conn)\n",
    "    stationData[[\"district\"]] = stationData[[\"district\"]].astype(int)\n",
    "\n",
    "    return stationData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateHlyData(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Aggregate the hourly weather station data by district, year, month and day\n",
    "\n",
    "    Psuedocode:\n",
    "    - Aggregate the columns by district, year, month and day\n",
    "    - Name the columns into the final DataFrame\n",
    "    \"\"\"\n",
    "    agg_df = (\n",
    "        df.groupby([\"district\", \"year\", \"month\", \"day\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"min_temp\": \"mean\",\n",
    "                \"max_temp\": \"mean\",\n",
    "                \"mean_temp\": \"mean\",\n",
    "                \"min_dew_point_temp\": \"mean\",\n",
    "                \"max_dew_point_temp\": \"mean\",\n",
    "                \"mean_dew_point_temp\": \"mean\",\n",
    "                \"min_humidex\": \"mean\",\n",
    "                \"max_humidex\": \"mean\",\n",
    "                \"mean_humidex\": \"mean\",\n",
    "                \"total_precip\": [\"min\", \"max\", \"mean\"],\n",
    "                \"min_rel_humid\": \"mean\",\n",
    "                \"max_rel_humid\": \"mean\",\n",
    "                \"mean_rel_humid\": \"mean\",\n",
    "                \"min_stn_press\": \"mean\",\n",
    "                \"max_stn_press\": \"mean\",\n",
    "                \"mean_stn_press\": \"mean\",\n",
    "                \"min_visibility\": \"mean\",\n",
    "                \"max_visibility\": \"mean\",\n",
    "                \"mean_visibility\": \"mean\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # sets the column names for the aggregate dataframe\n",
    "    agg_df.columns = [  # type: ignore\n",
    "        \"district\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"day\",\n",
    "        \"min_temp\",\n",
    "        \"max_temp\",\n",
    "        \"mean_temp\",\n",
    "        \"min_dew_point_temp\",\n",
    "        \"max_dew_point_temp\",\n",
    "        \"mean_dew_point_temp\",\n",
    "        \"min_humidex\",\n",
    "        \"max_humidex\",\n",
    "        \"mean_humidex\",\n",
    "        \"min_precip\",\n",
    "        \"max_precip\",\n",
    "        \"mean_precip\",\n",
    "        \"min_rel_humid\",\n",
    "        \"max_rel_humid\",\n",
    "        \"mean_rel_humid\",\n",
    "        \"min_stn_press\",\n",
    "        \"max_stn_press\",\n",
    "        \"mean_stn_press\",\n",
    "        \"min_visibility\",\n",
    "        \"max_visibility\",\n",
    "        \"mean_visibility\",\n",
    "    ]\n",
    "\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDates() -> list:\n",
    "    # figure out the date range for date processing - puts all 365 days as MO-DA into dates\n",
    "    dates = []  # all 365 days as MO-DA - strings\n",
    "\n",
    "    # the month range we want to pull data from - strings\n",
    "    months = [str(month) for month in range(MIN_MONTH, MAX_MONTH + 1)]\n",
    "    for month in months:\n",
    "        if len(month) == 1:\n",
    "            month = \"0\" + month\n",
    "\n",
    "        numDays = calendar.monthrange(2001, int(month))[1]\n",
    "        days = [str(day) for day in range(1, numDays + 1)]\n",
    "\n",
    "        for day in days:\n",
    "            if len(day) == 1:\n",
    "                day = \"0\" + day\n",
    "\n",
    "            dates.append(f\"{month}-{day}\")\n",
    "\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapeHlyData(\n",
    "    dates: list, agg_df: pd.DataFrame, stationData: pd.DataFrame\n",
    ") -> list:\n",
    "    # loads all data where each row is its own dictionary so that it may added to a dataframe later (fast processing)\n",
    "    listForDF = []\n",
    "\n",
    "    # the year range we want to pull data from - ints\n",
    "    years = [year for year in range(MIN_YEAR, MAX_YEAR + 1)]\n",
    "    uniqueDistricts = stationData[\"district\"].unique()\n",
    "\n",
    "    # get the columns we will want to pull information from\n",
    "    cols = agg_df.columns.tolist()  # type: ignore\n",
    "    cols.remove(\"district\")\n",
    "    cols.remove(\"year\")\n",
    "    cols.remove(\"month\")\n",
    "    cols.remove(\"day\")\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"Processing year: {year}\")\n",
    "\n",
    "        for district in uniqueDistricts:\n",
    "            currData = {}  # for each year/district combination create a dictionary\n",
    "\n",
    "            # adds the year and district\n",
    "            currData[\"year\"] = year\n",
    "            currData[\"district\"] = district\n",
    "\n",
    "            # for each day we want to grab all attributes and establish them as columns i.e MO-DA:attribute\n",
    "            for date in dates:\n",
    "                dateComponents = date.split(\"-\")\n",
    "                monthInt = int(dateComponents[0])\n",
    "                dayInt = int(dateComponents[1])\n",
    "\n",
    "                # grab the row from the aggregated df\n",
    "                currRow = agg_df.loc[\n",
    "                    (agg_df[\"year\"] == year)\n",
    "                    & (agg_df[\"month\"] == monthInt)\n",
    "                    & (agg_df[\"day\"] == dayInt)\n",
    "                    & (agg_df[\"district\"] == district)\n",
    "                ]\n",
    "\n",
    "                for col in cols:  # parse each of the desired columns\n",
    "                    currAttr = f\"{date}:{col}\"  # the current attribute which corresponds to the date and the column\n",
    "                    currVal = 0  # defaults as zero incase it does not exist\n",
    "\n",
    "                    if len(currRow[col]) == 1:\n",
    "                        # the current value from the loaded data\n",
    "                        currVal = currRow[col].item()\n",
    "\n",
    "                    currData[currAttr] = currVal\n",
    "\n",
    "            listForDF.append(currData)\n",
    "\n",
    "    return listForDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
