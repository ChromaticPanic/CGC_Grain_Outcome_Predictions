{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Objective of this notebook is to go through a list of weather stations and collect the weather data for each station. The data is collected from the Canada open data portal. Data for each station is combined into a single table then stored in a database for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 16:48:49.032148: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-21 16:48:49.152743: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pygeos as pg\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sqlalchemy as sq\n",
    "import ipyparallel as ipp\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from DataService import DataService\n",
    "from ClimateDataRequester import ClimateDataRequester as cdr\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.chdir('/tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PGUSER = os.getenv('POSTGRES_USER')\n",
    "PGPW = os.getenv('POSTGRES_PW')\n",
    "PGDB = os.getenv('POSTGRES_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataService = DataService(PGDB, PGUSER, PGPW)\n",
    "db_con = dataService.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_connection_url = \"postgresql://grpthreeuser:grpthreeuser@postgres:5432/grpthreedb\"\n",
    "# engine = sq.create_engine(db_connection_url)\n",
    "# db_con = engine.connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableName = \"public.\\\"lgFireStationsTen\\\"\"\n",
    "query = \"SELECT * FROM \" + tableName + \";\"\n",
    "dfStations = gpd.GeoDataFrame.from_postgis(query, db_con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_data(df: pd.DataFrame) -> None:\n",
    "    df.to_sql(\"WeatherData\", db_con, if_exists=\"append\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataProcessA(df: pd.DataFrame, stationID: str) -> None:\n",
    "    try:\n",
    "        df.drop(columns=['Data Quality', 'Max Temp Flag', 'Mean Temp Flag', 'Min Temp Flag', 'Heat Deg Days Flag', 'Cool Deg Days Flag', 'Spd of Max Gust (km/h)',\n",
    "                         'Total Rain Flag', 'Total Snow Flag', 'Total Precip Flag', 'Snow on Grnd Flag', 'Dir of Max Gust Flag', 'Spd of Max Gust Flag',\n",
    "                         'Heat Deg Days (°C)', 'Cool Deg Days (°C)', 'Longitude (x)', 'Latitude (y)', 'Station Name', 'Dir of Max Gust (10s deg)'], inplace=True)\n",
    "    except:\n",
    "        df.to_csv(\"Failed/\" + str(df.iloc[0, 0]) +\n",
    "                  \"_unexpected_column_names.csv\", index=False)\n",
    "\n",
    "    # Climate ID\tDate/Time\tYear\tMonth\tDay\tMax Temp (Â°C)\tMin Temp (Â°C)\tMean Temp (Â°C)\tTotal Rain (mm)\tTotal Snow (cm)\tTotal Precip (mm)\tSnow on Grnd (cm)\tDir of Max Gust (10s deg)\tSpd of Max Gust (km/h)\n",
    "    # ClimateID Date Year Month Day MaxTemp MinTemp MeanTemp TotalRain TotalSnow TotalPrecip SnowOnGrnd DirOfMaxGust SpdOfMaxGust\n",
    "    df.rename(columns={df.columns[0]: \"ClimateID\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[1]: \"Date\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[2]: \"Year\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[3]: \"Month\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[4]: \"Day\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[5]: \"MaxTemp\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[6]: \"MinTemp\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[7]: \"MeanTemp\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[8]: \"TotalRain\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[9]: \"TotalSnow\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[10]: \"TotalPrecip\"}, inplace=True)\n",
    "    df.rename(columns={df.columns[11]: \"SnowOnGrnd\"}, inplace=True)\n",
    "\n",
    "    df.dropna(subset=['MeanTemp'], inplace=True)\n",
    "    df.loc[df['SnowOnGrnd'].isnull(), 'SnowOnGrnd'] = 0\n",
    "    df.loc[df['TotalRain'].isnull(), 'TotalRain'] = 0\n",
    "    df.loc[df['TotalSnow'].isnull(), 'TotalSnow'] = 0\n",
    "    df.loc[df['TotalPrecip'].isnull(), 'TotalPrecip'] = 0\n",
    "    df['MaxTemp'] = np.where(df['MaxTemp'].isnull(),\n",
    "                             df['MeanTemp'], df['MaxTemp'])\n",
    "    df['MinTemp'] = np.where(df['MinTemp'].isnull(),\n",
    "                             df['MeanTemp'], df['MinTemp'])\n",
    "\n",
    "    df[['ClimateID', 'Date']] = df[['ClimateID', 'Date']].astype(str)\n",
    "    df[['Year', 'Month', 'Day']] = df[['Year', 'Month', 'Day']].astype(int)\n",
    "    df[['MaxTemp', 'MinTemp', 'MeanTemp', 'TotalRain', 'TotalSnow', 'TotalPrecip', 'SnowOnGrnd']] = df[[\n",
    "        'MaxTemp', 'MinTemp', 'MeanTemp', 'TotalRain', 'TotalSnow', 'TotalPrecip', 'SnowOnGrnd']].astype(float)\n",
    "\n",
    "    # we try a db push, but if it fails, we place the data in a csv file\n",
    "    # try:\n",
    "    push_data(df)\n",
    "    db_con.execute(\n",
    "        \"UPDATE public.\\\"TenYrStationsDaily\\\" SET \\\"dataAvailable\\\" = True WHERE \\\"ClimateID\\\" like CAST(\\'{}\\' AS TEXT);\".format(stationID))\n",
    "    # except:\n",
    "    #     df.to_csv(\"Failed/\" + str(df.iloc[0, 0]) +\n",
    "    #             \"_data_failed_dbpush.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 1018 of 1020\n",
      "Province not wanted: BC\n",
      "Processed row 1019 of 1020\n"
     ]
    }
   ],
   "source": [
    "requester = cdr()\n",
    "\n",
    "provinces = {'brit': \"BC\", 'albe': \"AB\", 'sask': \"SK\", 'mani': \"MB\", 'onta': \"ON\", 'queb': \"QC\",\n",
    "             'nuna': \"NU\", 'yuko': \"YT\", 'nort': \"NT\", 'newf': \"NL\", 'prin': \"PE\", 'nova': \"NS\", 'new ': \"NB\"}\n",
    "\n",
    "# for each station, we will request the weather data for the years 2010 to 2022\n",
    "df = pd.DataFrame()\n",
    "for index, row in dfStations.iterrows():\n",
    "\n",
    "    if row['dataAvailable'] != True:\n",
    "\n",
    "        key = row['Province'].lower()[:4]\n",
    "        province = provinces[key]\n",
    "\n",
    "        if province == \"AB\" or province == \"SK\" or province == \"MB\" or province == \"YT\":\n",
    "            stationID = str(row['ClimateID'])\n",
    "            df = requester.get_data(province, stationID, 2010, 2022)\n",
    "            clear_output(wait=False)\n",
    "\n",
    "            if not df.empty:\n",
    "                dataProcessA(df, stationID)\n",
    "            else:\n",
    "                db_con.execute(\"UPDATE public.\\\"TenYrStationsDaily\\\" SET \\\"dataAvailable\\\" = False WHERE \\\"ClimateID\\\" like CAST(\\'{}\\' AS TEXT);\".format(stationID))\n",
    "\n",
    "        else:\n",
    "            print(\"Province not wanted: \" + province)\n",
    "\n",
    "    else:\n",
    "        print(\"Data for station \" + str(row['ClimateID']) + \" already exists.\")\n",
    "\n",
    "    print(\"Processed row \" + str(index) + \" of \" + str(len(dfStations)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
