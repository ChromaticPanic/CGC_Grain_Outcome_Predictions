{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sq\n",
    "import geopandas as gpd  # type: ignore\n",
    "import pandas as pd  # type: ignore\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os, sys, calendar\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from Shared.DataService import DataService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MONTH = 1\n",
    "MAX_MONTH = 12\n",
    "\n",
    "MIN_YEAR = 1995\n",
    "MAX_YEAR = 2022\n",
    "TABLENAME = \"agg_dly_weather\"\n",
    "\n",
    "load_dotenv()\n",
    "PG_DB = os.getenv(\"POSTGRES_DB\")\n",
    "PG_ADDR = os.getenv(\"POSTGRES_ADDR\")\n",
    "PG_PORT = os.getenv(\"POSTGRES_PORT\")\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PW = os.getenv(\"POSTGRES_PW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullWeatherData(conn: sq.engine.Connection) -> pd.DataFrame:\n",
    "    # pulling weather station data from the database\n",
    "    weatherDataQuery = sq.text(\n",
    "        \"\"\"\n",
    "        SELECT * FROM public.ab_station_data\n",
    "        UNION\n",
    "        SELECT * FROM public.mb_station_data\n",
    "        UNION\n",
    "        SELECT * FROM public.sk_station_data;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    return pd.read_sql(weatherDataQuery, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullStationData(conn: sq.engine.Connection) -> pd.DataFrame:\n",
    "    # pulling station data from the database\n",
    "    stationDataQuery = sq.text(\n",
    "        \"\"\"\n",
    "        SELECT station_id, district FROM public.stations_dly\n",
    "        WHERE district IS NOT NULL;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    stationData = pd.read_sql(stationDataQuery, conn)\n",
    "    stationData[[\"district\"]] = stationData[[\"district\"]].astype(int)\n",
    "\n",
    "    return stationData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateDlyData(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # aggregate the values in the dataframe by date and district\n",
    "    agg_df = (\n",
    "        df.groupby([\"district\", \"date\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"max_temp\": \"mean\",\n",
    "                \"min_temp\": \"mean\",\n",
    "                \"mean_temp\": \"mean\",\n",
    "                \"total_rain\": [\"min\", \"max\", \"mean\"],\n",
    "                \"total_snow\": [\"min\", \"max\", \"mean\"],\n",
    "                \"total_precip\": [\"min\", \"max\", \"mean\"],\n",
    "                \"snow_on_grnd\": [\"min\", \"max\", \"mean\"],\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # sets the column names for the aggregate dataframe\n",
    "    agg_df.columns = [  # type: ignore\n",
    "        \"district\",\n",
    "        \"date\",\n",
    "        \"max_temp\",\n",
    "        \"min_temp\",\n",
    "        \"mean_temp\",\n",
    "        \"min_total_rain\",\n",
    "        \"max_total_rain\",\n",
    "        \"mean_total_rain\",\n",
    "        \"min_total_snow\",\n",
    "        \"max_total_snow\",\n",
    "        \"mean_total_snow\",\n",
    "        \"min_total_precip\",\n",
    "        \"max_total_precip\",\n",
    "        \"mean_total_precip\",\n",
    "        \"min_snow_on_grnd\",\n",
    "        \"max_snow_on_grnd\",\n",
    "        \"mean_snow_on_grnd\",\n",
    "    ]\n",
    "\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDates() -> list:\n",
    "    # figure out the date range for date processing - puts all 365 days as MO-DA into dates\n",
    "    dates = []  # all 365 days as MO-DA - strings\n",
    "\n",
    "    # the month range we want to pull data from - strings\n",
    "    months = [str(month) for month in range(MIN_MONTH, MAX_MONTH + 1)]\n",
    "    for month in months:\n",
    "        if len(month) == 1:\n",
    "            month = \"0\" + month\n",
    "\n",
    "        numDays = calendar.monthrange(2001, int(month))[1]\n",
    "        days = [str(day) for day in range(1, numDays + 1)]\n",
    "\n",
    "        for day in days:\n",
    "            if len(day) == 1:\n",
    "                day = \"0\" + day\n",
    "\n",
    "            dates.append(f\"{month}-{day}\")\n",
    "\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapeDlyData(\n",
    "    dates: list, agg_df: pd.DataFrame, stationData: pd.DataFrame\n",
    ") -> list:\n",
    "    # loads all data where each row is its own dictionary so that it may added to a dataframe later (fast processing)\n",
    "    listForDF = []\n",
    "\n",
    "    # the year range we want to pull data from - ints\n",
    "    years = [year for year in range(MIN_YEAR, MAX_YEAR + 1)]\n",
    "    uniqueDistricts = stationData[\"district\"].unique()\n",
    "\n",
    "    # get the columns we will want to pull information from\n",
    "    cols = agg_df.columns.tolist()  # type: ignore\n",
    "    cols.remove(\"district\")\n",
    "    cols.remove(\"date\")\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"Processing year: {year}\")\n",
    "\n",
    "        for district in uniqueDistricts:\n",
    "            currData = {}  # for each year/district combination create a dictionary\n",
    "\n",
    "            # adds the year and district\n",
    "            currData[\"year\"] = year\n",
    "            currData[\"district\"] = district\n",
    "\n",
    "            # for each day we want to grab all attributes and establish them as columns i.e MO-DA:attribute\n",
    "            for date in dates:\n",
    "                # calculates the date we are current processing\n",
    "                fullDate = np.datetime64(f\"{str(year)}-{date}\")\n",
    "\n",
    "                # grab the row from the aggregated df\n",
    "                currRow = agg_df.loc[\n",
    "                    (agg_df[\"date\"] == fullDate) & (agg_df[\"district\"] == district)\n",
    "                ]\n",
    "\n",
    "                for col in cols:  # parse each of the desired columns\n",
    "                    currAttr = f\"{date}:{col}\"  # the current attribute which corresponds to the date and the column\n",
    "                    currVal = 0  # defaults as zero incase it does not exist\n",
    "\n",
    "                    if len(currRow[col]) == 1:\n",
    "                        # the current value from the loaded data\n",
    "                        currVal = currRow[col].item()\n",
    "\n",
    "                    currData[currAttr] = currVal\n",
    "\n",
    "            listForDF.append(currData)\n",
    "\n",
    "    return listForDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if (\n",
    "        PG_DB is None\n",
    "        or PG_ADDR is None\n",
    "        or PG_PORT is None\n",
    "        or PG_USER is None\n",
    "        or PG_PW is None\n",
    "    ):\n",
    "        raise ValueError(\"Environment variables not set\")\n",
    "\n",
    "    db = DataService(PG_DB, PG_ADDR, int(PG_PORT), PG_USER, PG_PW)\n",
    "    conn = db.connect()\n",
    "\n",
    "    weatherData = pullWeatherData(conn)\n",
    "    stationData = pullStationData(conn)\n",
    "\n",
    "    # merge both the weather station data and the station data together\n",
    "    df = weatherData.merge(stationData, on=\"station_id\")\n",
    "\n",
    "    agg_df = aggregateDlyData(df)\n",
    "    dates = getDates()\n",
    "    listForDF = reshapeDlyData(dates, agg_df, stationData)\n",
    "    final_df = pd.DataFrame(listForDF)\n",
    "\n",
    "    try:\n",
    "        final_df.to_csv(path_or_buf='data/aggregatedDly.csv', sep=',', columns=final_df.columns.tolist())\n",
    "        # final_df.to_sql(TABLENAME, conn, schema=\"public\", if_exists=\"append\", index=False)\n",
    "    except Exception as e:\n",
    "        print('[ERROR]')\n",
    "        print(e)\n",
    "    \n",
    "    db.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year: 1995\n",
      "Processing year: 1996\n",
      "Processing year: 1997\n",
      "Processing year: 1998\n",
      "Processing year: 1999\n",
      "Processing year: 2000\n",
      "Processing year: 2001\n",
      "Processing year: 2002\n",
      "Processing year: 2003\n",
      "Processing year: 2004\n",
      "Processing year: 2005\n",
      "Processing year: 2006\n",
      "Processing year: 2007\n",
      "Processing year: 2008\n",
      "Processing year: 2009\n",
      "Processing year: 2010\n",
      "Processing year: 2011\n",
      "Processing year: 2012\n",
      "Processing year: 2013\n",
      "Processing year: 2014\n",
      "Processing year: 2015\n",
      "Processing year: 2016\n",
      "Processing year: 2017\n",
      "Processing year: 2018\n",
      "Processing year: 2019\n",
      "Processing year: 2020\n",
      "Processing year: 2021\n",
      "Processing year: 2022\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
